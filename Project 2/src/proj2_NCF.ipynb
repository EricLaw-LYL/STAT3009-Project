{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from developed_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def mahalanobis_method(df):\n",
    "    #M-Distance\n",
    "    x_minus_mu = df - np.mean(df)\n",
    "    cov = np.cov(df.values.T)                           #Covariance\n",
    "    inv_covmat = sp.linalg.inv(cov)                     #Inverse covariance\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat) \n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    md = np.sqrt(mahal.diagonal())\n",
    "    \n",
    "    #Flag as outlier\n",
    "    outlier = []\n",
    "    #Cut-off point\n",
    "    C = np.sqrt(chi2.ppf((1-0.001), df=df.shape[1]))    #degrees of freedom = number of variables\n",
    "    for index, value in enumerate(md):\n",
    "        if value > C:\n",
    "            outlier.append(index)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier, md\n",
    "\n",
    "# save the predicted ratings to csv file\n",
    "def save_csv(df, folder_path, method):\n",
    "    nowTime = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    fileName = \"{folder_path}/{method}_{nowTime}.csv\".format(folder_path = folder_path, method = method, nowTime = nowTime)\n",
    "    df.to_csv(fileName, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating = pd.read_csv(\"../data/train_rating.csv\")\n",
    "test_pair = pd.read_csv(\"../data/test_pair.csv\")\n",
    "\n",
    "item_feat = pd.read_csv(\"../data/item_feats.csv\")\n",
    "user_feat = pd.read_csv(\"../data/user_feats.csv\")\n",
    "\n",
    "sub = pd.read_csv('../predict/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserID\n",
    "le_user = preprocessing.LabelEncoder()\n",
    "le_user.fit(np.append(np.append(train_rating['UserId'], test_pair[\"UserId\"]), user_feat[\"UserId\"]))\n",
    "\n",
    "user_feat['UserId'] = le_user.transform(user_feat[\"UserId\"])\n",
    "test_pair[\"UserId\"] = le_user.transform(test_pair[\"UserId\"])\n",
    "train_rating['UserId'] = le_user.transform(train_rating[\"UserId\"])\n",
    "\n",
    "# ItemID\n",
    "le_item = preprocessing.LabelEncoder()\n",
    "le_item.fit(np.append(np.append(train_rating['ItemId'], test_pair[\"ItemId\"]), item_feat[\"ItemId\"]))\n",
    "\n",
    "item_feat['ItemId'] = le_item.transform(item_feat[\"ItemId\"])\n",
    "test_pair[\"ItemId\"] = le_item.transform(test_pair[\"ItemId\"])\n",
    "train_rating['ItemId'] = le_item.transform(train_rating[\"ItemId\"])\n",
    "\n",
    "#Inf value\n",
    "user_feat.loc[np.isinf(user_feat['V1']),'V1']=-3\n",
    "item_feat.loc[np.isinf(item_feat['V2']),'V2']=2\n",
    "\n",
    "# Missing data\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(user_feat['V1'].values.reshape(-1, 1))\n",
    "user_feat['V1'] = imp_mean.transform(user_feat['V1'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tran_pair, train_rating\n",
    "train_pairs = train_rating[['UserId', 'ItemId']].values\n",
    "train_ratings = train_rating['rating'].values\n",
    "train_pair=train_rating.drop(columns='rating')\n",
    "\n",
    "# test_pair\n",
    "test_pairs = test_pair[['UserId', 'ItemId']].values\n",
    "\n",
    "# number of users and items\n",
    "# n_user, n_item = len(le_item.classes_), len(le_item.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user, n_item = max(train_pairs[:,0].max(), test_pairs[:,0].max())+1, max(train_pairs[:,1].max(), test_pairs[:,1].max())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minmium and maximum: [0.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "class min_max_adj:\n",
    "    def __init__(self, train_rating):\n",
    "        self.min = np.min(train_rating)\n",
    "        self.max = np.max(train_rating)\n",
    "        self.true_rating = train_rating\n",
    "    \n",
    "    def adjust(self, pred_rating):\n",
    "        pred_rating_adjusted = pred_rating.copy()\n",
    "        pred_rating_adjusted[pred_rating > self.max] = self.max\n",
    "        pred_rating_adjusted[pred_rating < self.min] = self.min\n",
    "        return pred_rating_adjusted\n",
    "\n",
    "    def rmse(self, pred_rating):\n",
    "        return np.sqrt(np.mean((pred_rating - self.true_rating)**2))\n",
    "\n",
    "adjustment = min_max_adj(train_rating[\"rating\"])\n",
    "print(\"Minmium and maximum:\", [adjustment.min, adjustment.max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additonal features\n",
    "additional features and {rating_mean, rating_count}  \n",
    "\n",
    "## user_pd and item_pd\n",
    "using outer join and fill missing data  \n",
    "if no rating records, rating_count = 0 and rating_mean = glb_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "########## 10 random samples for users feats ##########\n",
      "#######################################################\n",
      "        UserId        V1      V2      V3     V4  rating_mean  rating_count\n",
      "8236   18617.0  3.389981  6579.0  1134.0  330.0     2.211891           0.0\n",
      "22416   5006.0  3.423729  3040.0  1079.0   56.0     2.211891           0.0\n",
      "7457    8995.0  3.423729  2249.0   626.0  330.0     2.211891           0.0\n",
      "1421   22188.0  3.139943   582.0   296.0  290.0     2.211891           0.0\n",
      "16075  12405.0  3.423729  5989.0   972.0  330.0     2.211891           0.0\n",
      "19253  19047.0  3.223776  8195.0  1630.0  300.0     2.211891           0.0\n",
      "19715  15778.0  3.447693  1491.0  1134.0  330.0     2.211891           0.0\n",
      "11122  16882.0  3.423729  6508.0  1555.0  330.0     0.000000           1.0\n",
      "2386   13911.0  3.423729  1489.0   439.0  325.0     2.211891           0.0\n",
      "25946   8164.0  3.423729  5654.0  1134.0  330.0     2.211891           0.0\n",
      "#######################################################\n",
      "########## 10 random samples for items feats ##########\n",
      "#######################################################\n",
      "        ItemId       V1        V2      V3  rating_mean  rating_count\n",
      "17839  25686.0    799.0  7.492632  2188.0     1.500000           2.0\n",
      "6290   16323.0  12562.0  7.566990  1197.0     1.612626           0.0\n",
      "16943  12621.0    552.0  7.504492  1202.0     1.612626           0.0\n",
      "6406    4923.0  11424.0  7.576602  2115.0     4.166667           3.0\n",
      "19922  18308.0    650.0  7.588328  2129.0     1.612626           0.0\n",
      "15892    387.0   9140.0  7.672250  2319.0     1.875000           4.0\n",
      "17003   8839.0   8133.0  7.590207   125.0     4.000000           1.0\n",
      "3828   14033.0  11639.0  7.586334  2560.0     1.125000           8.0\n",
      "18191  21116.0    709.0  7.664878  1214.0     0.000000           1.0\n",
      "2148   16744.0   8507.0  7.483364  2570.0     0.000000           1.0\n"
     ]
    }
   ],
   "source": [
    "## generate cont feats for users\n",
    "user_pd = pd.merge(left=train_rating.groupby('UserId')['rating'].mean(), \n",
    "\t\t\t\t   right=train_rating.groupby('UserId')['rating'].count(), on='UserId')\n",
    "user_pd.columns = ['rating_mean', 'rating_count']\n",
    "user_pd = pd.merge(left = user_feat, right = user_pd, on = \"UserId\", how = \"outer\") # using outer join\n",
    "\n",
    "## handle missing data\n",
    "# if the user has no rating record, set rating_count = 0\n",
    "user_pd.fillna(value = {\"rating_count\": 0}, inplace = True)\n",
    "# if the rating_mean is missing, then use global mean\n",
    "imp_mean.fit(user_pd)\n",
    "user_pd = pd.DataFrame(imp_mean.transform(user_pd), columns = user_pd.columns)\n",
    "\n",
    "## generate cont feats for items\n",
    "item_rating_pd = pd.merge(left=train_rating.groupby('ItemId')['rating'].mean(), \n",
    "\t\t\t\t\t\t  right=train_rating.groupby('ItemId')['rating'].count(), on='ItemId')\n",
    "item_rating_pd.columns\t= ['rating_mean', 'rating_count']\n",
    "item_pd = pd.merge(left=item_feat, right=item_rating_pd, on='ItemId', how = \"outer\") # using outer join\n",
    "\n",
    "## handle missing data\n",
    "# if the item has no rating record, set rating_count = 0\n",
    "item_pd.fillna(value = {\"rating_count\": 0}, inplace = True)\n",
    "# if the rating_mean is missing, then use global mean\n",
    "imp_mean.fit(item_pd)\n",
    "item_pd = pd.DataFrame(imp_mean.transform(item_pd), columns = item_pd.columns)\n",
    "\n",
    "\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for users feats ##########')\n",
    "print('#######################################################')\n",
    "\n",
    "print(user_pd.sample(10))\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for items feats ##########')\n",
    "print('#######################################################')\n",
    "\n",
    "print(item_pd.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize continous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "########## 10 random samples for users feats ##########\n",
      "#######################################################\n",
      "               V1        V2        V3        V4   rating_mean  rating_count\n",
      "UserId                                                                     \n",
      "4524.0   0.000000 -0.893384 -0.266972  0.762978 -5.564071e-16     -0.067335\n",
      "20398.0  0.589498  0.628581 -1.572501  0.762978 -5.564071e-16     -0.067335\n",
      "20655.0  0.025396 -1.478170 -1.456652  0.412484 -5.564071e-16     -0.067335\n",
      "14222.0 -0.769082  0.534778  0.838050 -1.147216 -5.564071e-16     -0.067335\n",
      "15450.0  0.926189 -1.707606  0.477135 -1.068355 -5.564071e-16     -0.067335\n",
      "15686.0 -0.531399  0.906185  0.131816  0.762978 -5.564071e-16     -0.067335\n",
      "19959.0  0.000000  1.218860  0.289995  0.482583 -5.564071e-16     -0.067335\n",
      "12507.0  0.000000  0.758721  0.844733 -1.637908 -5.564071e-16     -0.067335\n",
      "3771.0   0.480938 -1.213242  0.588529  0.762978 -5.564071e-16     -0.067335\n",
      "10696.0 -0.858740 -0.470851 -0.585556 -0.998256  2.240353e+00     -0.022246\n",
      "#######################################################\n",
      "########## 10 random samples for items feats ##########\n",
      "#######################################################\n",
      "               V1        V2        V3  rating_mean  rating_count\n",
      "ItemId                                                          \n",
      "14407.0 -1.559266  0.047883  0.938267    -1.036886      0.173125\n",
      "17019.0  1.125517 -0.069989 -1.417580    -1.036886     -0.212289\n",
      "7305.0  -0.610245  0.098467 -0.234422    -1.036886     -0.212289\n",
      "17323.0 -0.047037  0.119528 -0.412419    -1.036886     -0.212289\n",
      "23067.0 -0.594335  0.134792  0.448250     1.535033     -0.212289\n",
      "1817.0   0.482495  0.031084  0.890103    -1.036886     -0.212289\n",
      "12175.0  1.085478  0.068938 -0.016637     0.000000     -0.597702\n",
      "2855.0  -0.423569 -0.032701  0.682788     1.213543     -0.212289\n",
      "19369.0  0.335064  0.265150 -0.799825    -1.036886     -0.212289\n",
      "23264.0 -0.423569 -0.134480  0.682788     0.000000     -0.597702\n"
     ]
    }
   ],
   "source": [
    "## pre-processing for users\n",
    "user_cont = [\"V1\", \"V2\", \"V3\", \"V4\", \"rating_mean\", \"rating_count\"]\n",
    "user_pd[user_cont] = StandardScaler().fit_transform(user_pd[user_cont])\n",
    "\n",
    "## pre-processing for item\n",
    "item_cont = [\"V1\", \"V2\", \"V3\", \"rating_mean\", \"rating_count\"]\n",
    "item_pd[item_cont] = StandardScaler().fit_transform(item_pd[item_cont])\n",
    "\n",
    "\n",
    "user_pd = user_pd.set_index('UserId', drop=True)\n",
    "item_pd = item_pd.set_index('ItemId', drop=True)\n",
    "\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for users feats ##########')\n",
    "print('#######################################################')\n",
    "print(user_pd.sample(10))\n",
    "\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for items feats ##########')\n",
    "print('#######################################################')\n",
    "print(item_pd.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF model\n",
    "only two embeddings for categorical features, UserId and ItemId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SideNCF(keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size, **kwargs):\n",
    "        super(SideNCF, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.itme_embedding = layers.Embedding(\n",
    "            num_items,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "\n",
    "        self.concatenate = layers.Concatenate()\n",
    "        self.dense1 = layers.Dense(100, name='fc-1', activation='relu')\n",
    "        self.dense2 = layers.Dense(50, name='fc-2', activation='relu')\n",
    "        self.dense3 = layers.Dense(1, name='fc-3', activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cont_feats = inputs[0]\n",
    "        cate_feats = inputs[1]\n",
    "\n",
    "        user_vector = self.user_embedding(cate_feats[:,0])\n",
    "        itme_vector = self.itme_embedding(cate_feats[:,1])\n",
    "\n",
    "        concatted_vec = self.concatenate([cont_feats, user_vector, itme_vector])\n",
    "        fc_1 = self.dense1(concatted_vec)\n",
    "        fc_2 = self.dense2(fc_1)\n",
    "        fc_3 = self.dense3(fc_2)\n",
    "        return fc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SideNCF(num_users=n_user, num_items=n_item, embedding_size=50)\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "    keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3), \n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the continuous features and categorical features for user and item, respectively\n",
    "cate_feats = [\"UserId\", \"ItemId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cont = [\"V1\", \"V2\", \"V3\", 'rating_mean', 'rating_count']\n",
    "user_cont = [\"V1\", \"V2\", \"V3\", \"V4\", 'rating_mean', 'rating_count']\n",
    "\n",
    "train_cont_feats = np.hstack((user_pd.loc[train_pairs[:,0]][user_cont], item_pd.loc[train_pairs[:,1]][item_cont]))\n",
    "train_cate_feats = train_pairs.copy()\n",
    "\n",
    "test_cont_feats = np.hstack((user_pd.loc[test_pairs[:,0]][user_cont], item_pd.loc[test_pairs[:,1]][item_cont]))\n",
    "test_cate_feats = test_pairs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "504/504 [==============================] - 19s 35ms/step - loss: 1.9452 - mae: 0.8944 - rmse: 1.3736 - val_loss: 1.5330 - val_mae: 0.7570 - val_rmse: 1.2214\n",
      "Epoch 2/50\n",
      "504/504 [==============================] - 15s 30ms/step - loss: 1.5690 - mae: 0.7595 - rmse: 1.2359 - val_loss: 1.4734 - val_mae: 0.7564 - val_rmse: 1.1975\n",
      "Epoch 3/50\n",
      "504/504 [==============================] - 15s 29ms/step - loss: 1.5135 - mae: 0.7386 - rmse: 1.2120 - val_loss: 1.4567 - val_mae: 0.7437 - val_rmse: 1.1911\n",
      "Epoch 4/50\n",
      "504/504 [==============================] - 16s 31ms/step - loss: 1.5186 - mae: 0.7341 - rmse: 1.2152 - val_loss: 1.4493 - val_mae: 0.7288 - val_rmse: 1.1853\n",
      "Epoch 5/50\n",
      "504/504 [==============================] - 18s 36ms/step - loss: 1.4433 - mae: 0.7195 - rmse: 1.1796 - val_loss: 1.4622 - val_mae: 0.7437 - val_rmse: 1.1893\n",
      "Epoch 6/50\n",
      "504/504 [==============================] - 20s 40ms/step - loss: 1.4342 - mae: 0.7117 - rmse: 1.1725 - val_loss: 1.4564 - val_mae: 0.7227 - val_rmse: 1.1825\n",
      "Epoch 7/50\n",
      "504/504 [==============================] - 25s 49ms/step - loss: 1.4174 - mae: 0.7079 - rmse: 1.1635 - val_loss: 1.5339 - val_mae: 0.7619 - val_rmse: 1.2122\n",
      "Epoch 8/50\n",
      "504/504 [==============================] - 33s 66ms/step - loss: 1.4245 - mae: 0.7059 - rmse: 1.1629 - val_loss: 1.4799 - val_mae: 0.7331 - val_rmse: 1.1868\n",
      "Epoch 9/50\n",
      "504/504 [==============================] - 20s 39ms/step - loss: 1.4308 - mae: 0.7027 - rmse: 1.1611 - val_loss: 1.4823 - val_mae: 0.7325 - val_rmse: 1.1849\n",
      "Epoch 10/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.4179 - mae: 0.6993 - rmse: 1.1526 - val_loss: 1.4853 - val_mae: 0.7047 - val_rmse: 1.1827\n",
      "Epoch 11/50\n",
      "504/504 [==============================] - 16s 32ms/step - loss: 1.4120 - mae: 0.6891 - rmse: 1.1439 - val_loss: 1.5066 - val_mae: 0.7376 - val_rmse: 1.1846\n",
      "Epoch 12/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.4284 - mae: 0.6881 - rmse: 1.1426 - val_loss: 1.5111 - val_mae: 0.7127 - val_rmse: 1.1801\n",
      "Epoch 13/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 1.4021 - mae: 0.6745 - rmse: 1.1247 - val_loss: 1.5475 - val_mae: 0.7170 - val_rmse: 1.1869\n",
      "Epoch 14/50\n",
      "504/504 [==============================] - 17s 34ms/step - loss: 1.3953 - mae: 0.6619 - rmse: 1.1117 - val_loss: 1.5768 - val_mae: 0.7102 - val_rmse: 1.1910\n",
      "Epoch 15/50\n",
      "504/504 [==============================] - 17s 35ms/step - loss: 1.3749 - mae: 0.6423 - rmse: 1.0943 - val_loss: 1.6353 - val_mae: 0.7177 - val_rmse: 1.2062\n",
      "Epoch 16/50\n",
      "504/504 [==============================] - 17s 33ms/step - loss: 1.3442 - mae: 0.6202 - rmse: 1.0723 - val_loss: 1.7209 - val_mae: 0.7304 - val_rmse: 1.2334\n",
      "Epoch 17/50\n",
      "504/504 [==============================] - 15s 29ms/step - loss: 1.3220 - mae: 0.6046 - rmse: 1.0552 - val_loss: 1.8059 - val_mae: 0.7640 - val_rmse: 1.2599\n",
      "Epoch 18/50\n",
      "504/504 [==============================] - 17s 34ms/step - loss: 1.2868 - mae: 0.5832 - rmse: 1.0304 - val_loss: 1.8025 - val_mae: 0.7376 - val_rmse: 1.2524\n",
      "Epoch 19/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.2692 - mae: 0.5677 - rmse: 1.0141 - val_loss: 1.8316 - val_mae: 0.7197 - val_rmse: 1.2572\n",
      "Epoch 20/50\n",
      "504/504 [==============================] - 21s 41ms/step - loss: 1.2393 - mae: 0.5495 - rmse: 0.9932 - val_loss: 1.9329 - val_mae: 0.7445 - val_rmse: 1.2907\n",
      "Epoch 21/50\n",
      "504/504 [==============================] - 26s 52ms/step - loss: 1.2125 - mae: 0.5326 - rmse: 0.9752 - val_loss: 1.9671 - val_mae: 0.7361 - val_rmse: 1.3003\n",
      "Epoch 22/50\n",
      "504/504 [==============================] - 17s 33ms/step - loss: 1.1767 - mae: 0.5147 - rmse: 0.9523 - val_loss: 2.0555 - val_mae: 0.7404 - val_rmse: 1.3282\n",
      "Epoch 23/50\n",
      "504/504 [==============================] - 17s 33ms/step - loss: 1.1712 - mae: 0.5021 - rmse: 0.9439 - val_loss: 2.1106 - val_mae: 0.7584 - val_rmse: 1.3431\n",
      "Epoch 24/50\n",
      "504/504 [==============================] - 15s 29ms/step - loss: 1.1294 - mae: 0.4830 - rmse: 0.9184 - val_loss: 2.0963 - val_mae: 0.7309 - val_rmse: 1.3375\n",
      "Epoch 25/50\n",
      "504/504 [==============================] - 17s 34ms/step - loss: 1.0946 - mae: 0.4629 - rmse: 0.8980 - val_loss: 2.1252 - val_mae: 0.7359 - val_rmse: 1.3481\n",
      "Epoch 26/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.0750 - mae: 0.4543 - rmse: 0.8848 - val_loss: 2.2177 - val_mae: 0.7575 - val_rmse: 1.3809\n",
      "Epoch 27/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.0737 - mae: 0.4501 - rmse: 0.8808 - val_loss: 2.2026 - val_mae: 0.7440 - val_rmse: 1.3701\n",
      "Epoch 28/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 1.0452 - mae: 0.4346 - rmse: 0.8615 - val_loss: 2.2122 - val_mae: 0.7400 - val_rmse: 1.3738\n",
      "Epoch 29/50\n",
      "504/504 [==============================] - 14s 27ms/step - loss: 1.0266 - mae: 0.4217 - rmse: 0.8511 - val_loss: 2.2598 - val_mae: 0.7503 - val_rmse: 1.3906\n",
      "Epoch 30/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 1.0206 - mae: 0.4160 - rmse: 0.8449 - val_loss: 2.2929 - val_mae: 0.7545 - val_rmse: 1.4000\n",
      "Epoch 31/50\n",
      "504/504 [==============================] - 13s 25ms/step - loss: 0.9982 - mae: 0.4054 - rmse: 0.8309 - val_loss: 2.3535 - val_mae: 0.7659 - val_rmse: 1.4219\n",
      "Epoch 32/50\n",
      "504/504 [==============================] - 14s 27ms/step - loss: 0.9705 - mae: 0.3946 - rmse: 0.8149 - val_loss: 2.2722 - val_mae: 0.7388 - val_rmse: 1.3916\n",
      "Epoch 33/50\n",
      "504/504 [==============================] - 16s 31ms/step - loss: 0.9764 - mae: 0.3931 - rmse: 0.8155 - val_loss: 2.3248 - val_mae: 0.7417 - val_rmse: 1.4097\n",
      "Epoch 34/50\n",
      "504/504 [==============================] - 16s 32ms/step - loss: 0.9533 - mae: 0.3832 - rmse: 0.8010 - val_loss: 2.3022 - val_mae: 0.7350 - val_rmse: 1.4038\n",
      "Epoch 35/50\n",
      "504/504 [==============================] - 15s 31ms/step - loss: 0.9585 - mae: 0.3826 - rmse: 0.8030 - val_loss: 2.2750 - val_mae: 0.7336 - val_rmse: 1.3913\n",
      "Epoch 36/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 0.9440 - mae: 0.3764 - rmse: 0.7907 - val_loss: 2.3891 - val_mae: 0.7517 - val_rmse: 1.4283\n",
      "Epoch 37/50\n",
      "504/504 [==============================] - 15s 29ms/step - loss: 0.9500 - mae: 0.3776 - rmse: 0.7940 - val_loss: 2.4032 - val_mae: 0.7464 - val_rmse: 1.4337\n",
      "Epoch 38/50\n",
      "504/504 [==============================] - 14s 29ms/step - loss: 0.9299 - mae: 0.3667 - rmse: 0.7804 - val_loss: 2.3857 - val_mae: 0.7473 - val_rmse: 1.4286\n",
      "Epoch 39/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 0.9320 - mae: 0.3655 - rmse: 0.7809 - val_loss: 2.3277 - val_mae: 0.7302 - val_rmse: 1.4078\n",
      "Epoch 40/50\n",
      "504/504 [==============================] - 15s 29ms/step - loss: 0.9167 - mae: 0.3584 - rmse: 0.7727 - val_loss: 2.3221 - val_mae: 0.7467 - val_rmse: 1.4077\n",
      "Epoch 41/50\n",
      "504/504 [==============================] - 12s 24ms/step - loss: 0.9158 - mae: 0.3582 - rmse: 0.7685 - val_loss: 2.4048 - val_mae: 0.7401 - val_rmse: 1.4332\n",
      "Epoch 42/50\n",
      "504/504 [==============================] - 14s 27ms/step - loss: 0.8931 - mae: 0.3490 - rmse: 0.7568 - val_loss: 2.3693 - val_mae: 0.7400 - val_rmse: 1.4222\n",
      "Epoch 43/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 0.8775 - mae: 0.3421 - rmse: 0.7486 - val_loss: 2.4081 - val_mae: 0.7580 - val_rmse: 1.4387\n",
      "Epoch 44/50\n",
      "504/504 [==============================] - 13s 27ms/step - loss: 0.8751 - mae: 0.3438 - rmse: 0.7471 - val_loss: 2.3411 - val_mae: 0.7243 - val_rmse: 1.4127\n",
      "Epoch 45/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 0.8750 - mae: 0.3398 - rmse: 0.7447 - val_loss: 2.3875 - val_mae: 0.7348 - val_rmse: 1.4282\n",
      "Epoch 46/50\n",
      "504/504 [==============================] - 15s 30ms/step - loss: 0.8808 - mae: 0.3396 - rmse: 0.7479 - val_loss: 2.5008 - val_mae: 0.7636 - val_rmse: 1.4688\n",
      "Epoch 47/50\n",
      "504/504 [==============================] - 14s 29ms/step - loss: 0.8730 - mae: 0.3366 - rmse: 0.7395 - val_loss: 2.4143 - val_mae: 0.7439 - val_rmse: 1.4366\n",
      "Epoch 48/50\n",
      "504/504 [==============================] - 16s 32ms/step - loss: 0.8636 - mae: 0.3330 - rmse: 0.7349 - val_loss: 2.4405 - val_mae: 0.7317 - val_rmse: 1.4436\n",
      "Epoch 49/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 0.8581 - mae: 0.3304 - rmse: 0.7336 - val_loss: 2.4709 - val_mae: 0.7473 - val_rmse: 1.4579\n",
      "Epoch 50/50\n",
      "504/504 [==============================] - 16s 33ms/step - loss: 0.8541 - mae: 0.3268 - rmse: 0.7303 - val_loss: 2.4531 - val_mae: 0.7427 - val_rmse: 1.4548\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping( \n",
    "    monitor='val_rmse', min_delta=0, patience=5, verbose=1, \n",
    "    mode='auto', baseline=None, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    x=[train_cont_feats, train_cate_feats],\n",
    "    y=train_ratings,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_split=.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of training dataset\n",
    "the result is adjusted for min=0, max=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.       0.       0.       ... 0.       0.       4.631906]\n",
      "rmse: SideNCF: 0.874\n"
     ]
    }
   ],
   "source": [
    "pred_rating = model.predict([train_cont_feats, train_cate_feats]).flatten()\n",
    "pred_rating = adjustment.adjust(pred_rating)\n",
    "print(pred_rating)\n",
    "print('rmse: SideNCF: %.3f' %rmse(train_ratings, pred_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([test_cont_feats, test_cate_feats]).flatten()\n",
    "pred = adjustment.adjust(pred)\n",
    "sub[\"rating\"] = pred\n",
    "save_csv(sub, \"../predict\", \"NCF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the regressor using RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 500, random_state = 3009)\n",
    "regressor.fit(train_cate_feats, train_ratings)\n",
    "pred_rating = regressor.predict(test_cate_feats)\n",
    "\n",
    "pred = adjustment.adjust(pred_rating)\n",
    "sub[\"rating\"] = pred\n",
    "save_csv(sub, \"../predict\", \"rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF + Random Forest\n",
    "Train a random forest regressor by the residuals of NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rating = model.predict([train_cont_feats, train_cate_feats]).flatten()\n",
    "pred_rating = adjustment.adjust(pred_rating)\n",
    "train_ratings_cm = train_ratings - pred_rating\n",
    "\n",
    "NCF_regressor = RandomForestRegressor(n_estimators = 500, random_state = 3009)\n",
    "NCF_regressor.fit(train_cate_feats, train_ratings_cm)\n",
    "pred_rating = regressor.predict(test_cate_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rating = NCF_regressor.predict(test_cate_feats)\n",
    "pred = adjustment.adjust(pred_rating)\n",
    "sub[\"rating\"] = pred\n",
    "save_csv(sub, \"../predict\", \"NCF_rf\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5129930097a52138fdc5ab816b09a2f27e944ad02f83c97d5e0e22f93f3b8c3c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
