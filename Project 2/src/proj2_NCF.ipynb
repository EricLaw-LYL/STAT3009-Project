{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from developed_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def mahalanobis_method(df):\n",
    "    #M-Distance\n",
    "    x_minus_mu = df - np.mean(df)\n",
    "    cov = np.cov(df.values.T)                           #Covariance\n",
    "    inv_covmat = sp.linalg.inv(cov)                     #Inverse covariance\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat) \n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    md = np.sqrt(mahal.diagonal())\n",
    "    \n",
    "    #Flag as outlier\n",
    "    outlier = []\n",
    "    #Cut-off point\n",
    "    C = np.sqrt(chi2.ppf((1-0.001), df=df.shape[1]))    #degrees of freedom = number of variables\n",
    "    for index, value in enumerate(md):\n",
    "        if value > C:\n",
    "            outlier.append(index)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier, md\n",
    "\n",
    "# save the predicted ratings to csv file\n",
    "def save_csv(df, folder_path, method):\n",
    "    nowTime = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    fileName = \"{folder_path}/{method}_{nowTime}.csv\".format(folder_path = folder_path, method = method, nowTime = nowTime)\n",
    "    df.to_csv(fileName, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating = pd.read_csv(\"../data/train_rating.csv\")\n",
    "test_pair = pd.read_csv(\"../data/test_pair.csv\")\n",
    "\n",
    "item_feat = pd.read_csv(\"../data/item_feats.csv\")\n",
    "user_feat = pd.read_csv(\"../data/user_feats.csv\")\n",
    "\n",
    "sub = pd.read_csv('../predict/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserID\n",
    "le_user = preprocessing.LabelEncoder()\n",
    "le_user.fit(np.append(np.append(train_rating['UserId'], test_pair[\"UserId\"]), user_feat[\"UserId\"]))\n",
    "\n",
    "user_feat['UserId'] = le_user.transform(user_feat[\"UserId\"])\n",
    "test_pair[\"UserId\"] = le_user.transform(test_pair[\"UserId\"])\n",
    "train_rating['UserId'] = le_user.transform(train_rating[\"UserId\"])\n",
    "\n",
    "# ItemID\n",
    "le_item = preprocessing.LabelEncoder()\n",
    "le_item.fit(np.append(np.append(train_rating['ItemId'], test_pair[\"ItemId\"]), item_feat[\"ItemId\"]))\n",
    "\n",
    "item_feat['ItemId'] = le_item.transform(item_feat[\"ItemId\"])\n",
    "test_pair[\"ItemId\"] = le_item.transform(test_pair[\"ItemId\"])\n",
    "train_rating['ItemId'] = le_item.transform(train_rating[\"ItemId\"])\n",
    "\n",
    "#Inf value\n",
    "user_feat.loc[np.isinf(user_feat['V1']),'V1']=-3\n",
    "item_feat.loc[np.isinf(item_feat['V2']),'V2']=2\n",
    "\n",
    "# Missing data\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(user_feat['V1'].values.reshape(-1, 1))\n",
    "user_feat['V1'] = imp_mean.transform(user_feat['V1'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tran_pair, train_rating\n",
    "train_pairs = train_rating[['UserId', 'ItemId']].values\n",
    "train_ratings = train_rating['rating'].values\n",
    "train_pair=train_rating.drop(columns='rating')\n",
    "\n",
    "# test_pair\n",
    "test_pairs = test_pair[['UserId', 'ItemId']].values\n",
    "\n",
    "# number of users and items\n",
    "# n_user, n_item = len(le_item.classes_), len(le_item.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user, n_item = max(train_pairs[:,0].max(), test_pairs[:,0].max())+1, max(train_pairs[:,1].max(), test_pairs[:,1].max())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minmium and maximum: [0.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "class min_max_adj:\n",
    "    def __init__(self, train_rating):\n",
    "        self.min = np.min(train_rating)\n",
    "        self.max = np.max(train_rating)\n",
    "        self.true_rating = train_rating\n",
    "    \n",
    "    def adjust(self, pred_rating):\n",
    "        pred_rating_adjusted = pred_rating.copy()\n",
    "        pred_rating_adjusted[pred_rating > self.max] = self.max\n",
    "        pred_rating_adjusted[pred_rating < self.min] = self.min\n",
    "        return pred_rating_adjusted\n",
    "\n",
    "    def rmse(self, pred_rating):\n",
    "        return np.sqrt(np.mean((pred_rating - self.true_rating)**2))\n",
    "\n",
    "adjustment = min_max_adj(train_rating[\"rating\"])\n",
    "print(\"Minmium and maximum:\", [adjustment.min, adjustment.max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additonal features\n",
    "additional features and {rating_mean, rating_count}  \n",
    "\n",
    "## user_pd and item_pd\n",
    "using outer join and fill missing data  \n",
    "if no rating records, rating_count = 0 and rating_mean = glb_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "########## 10 random samples for users feats ##########\n",
      "#######################################################\n",
      "        UserId        V1      V2      V3     V4  rating_mean  rating_count\n",
      "16395  13409.0  3.423729  7314.0  1079.0   56.0     2.211891           0.0\n",
      "26853   9286.0  2.782265  6284.0  1422.0  330.0     2.211891           0.0\n",
      "19434  25184.0  3.744258  5981.0   843.0  330.0     2.211891           0.0\n",
      "24352  22760.0  3.884990  5511.0  1079.0   56.0     2.211891           0.0\n",
      "17803  18251.0  3.839146  3308.0   982.0  330.0     2.211891           0.0\n",
      "13249   9309.0  2.902763  2569.0  1421.0  330.0     2.211891           0.0\n",
      "18579  10146.0  2.699768  6022.0  1529.0  330.0     2.211891           0.0\n",
      "939    13459.0  3.423729  7483.0   621.0  330.0     2.211891           0.0\n",
      "6375   14906.0  3.423729  5320.0   961.0  330.0     5.000000           1.0\n",
      "3547    8090.0  3.423729  6347.0   621.0  330.0     2.211891           0.0\n",
      "#######################################################\n",
      "########## 10 random samples for items feats ##########\n",
      "#######################################################\n",
      "        ItemId       V1        V2      V3  rating_mean  rating_count\n",
      "16716   8113.0  12938.0  7.540469  1384.0     1.612626           0.0\n",
      "20600   3230.0  11051.0  7.508027  1780.0     1.612626           0.0\n",
      "3289   15748.0   2551.0  7.466703   701.0     1.875000           4.0\n",
      "38      3318.0   7197.0  7.589223   254.0     1.468750          16.0\n",
      "23503   2230.0   2955.0  7.674319  2655.0     0.000000           1.0\n",
      "14030   8465.0   2587.0  7.500539  3012.0     2.250000           2.0\n",
      "12285   6424.0   3454.0  7.347482   739.0     1.750000           2.0\n",
      "9536   15926.0   6971.0  7.563199  2973.0     1.500000           9.0\n",
      "12907  15196.0  12551.0  7.547354  1829.0     0.000000           1.0\n",
      "13071   5238.0   5205.0  7.693481   231.0     0.000000           1.0\n"
     ]
    }
   ],
   "source": [
    "## generate cont feats for users\n",
    "user_pd = pd.merge(left=train_rating.groupby('UserId')['rating'].mean(), \n",
    "\t\t\t\t   right=train_rating.groupby('UserId')['rating'].count(), on='UserId')\n",
    "user_pd.columns = ['rating_mean', 'rating_count']\n",
    "user_pd = pd.merge(left = user_feat, right = user_pd, on = \"UserId\", how = \"outer\") # using outer join\n",
    "\n",
    "## handle missing data\n",
    "# if the user has no rating record, set rating_count = 0\n",
    "user_pd.fillna(value = {\"rating_count\": 0}, inplace = True)\n",
    "# if the rating_mean is missing, then use global mean\n",
    "imp_mean.fit(user_pd)\n",
    "user_pd = pd.DataFrame(imp_mean.transform(user_pd), columns = user_pd.columns)\n",
    "\n",
    "## generate cont feats for items\n",
    "item_rating_pd = pd.merge(left=train_rating.groupby('ItemId')['rating'].mean(), \n",
    "\t\t\t\t\t\t  right=train_rating.groupby('ItemId')['rating'].count(), on='ItemId')\n",
    "item_rating_pd.columns\t= ['rating_mean', 'rating_count']\n",
    "item_pd = pd.merge(left=item_feat, right=item_rating_pd, on='ItemId', how = \"outer\") # using outer join\n",
    "\n",
    "## handle missing data\n",
    "# if the item has no rating record, set rating_count = 0\n",
    "item_pd.fillna(value = {\"rating_count\": 0}, inplace = True)\n",
    "# if the rating_mean is missing, then use global mean\n",
    "imp_mean.fit(item_pd)\n",
    "item_pd = pd.DataFrame(imp_mean.transform(item_pd), columns = item_pd.columns)\n",
    "\n",
    "\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for users feats ##########')\n",
    "print('#######################################################')\n",
    "\n",
    "print(user_pd.sample(10))\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for items feats ##########')\n",
    "print('#######################################################')\n",
    "\n",
    "print(item_pd.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize continous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "########## 10 random samples for users feats ##########\n",
      "#######################################################\n",
      "               V1        V2        V3        V4   rating_mean  rating_count\n",
      "UserId                                                                     \n",
      "13080.0 -0.329399 -1.579156  0.183057  0.762978 -5.564071e-16     -0.067335\n",
      "24236.0  0.000000 -1.492959  0.191969  0.762978 -5.564071e-16     -0.067335\n",
      "10500.0 -0.231626 -0.805920  1.744791  0.762978 -5.564071e-16     -0.067335\n",
      "471.0    0.000000 -1.349298 -0.745963  0.762978 -5.564071e-16     -0.067335\n",
      "11199.0  0.000000 -0.290429 -0.405099 -2.128601 -1.746202e+00      0.428649\n",
      "10260.0  0.000000  0.882101  0.131816  0.762978 -5.564071e-16     -0.067335\n",
      "21651.0 -0.662294 -1.697043  0.463768 -1.375038 -5.564071e-16     -0.067335\n",
      "11519.0 -0.769668 -0.852399 -0.870723  0.710404 -5.564071e-16     -0.067335\n",
      "6722.0   0.392024 -1.087327  1.410611 -0.779197 -5.564071e-16     -0.067335\n",
      "25128.0  0.000000  0.026048 -0.068692 -0.586425 -5.564071e-16     -0.067335\n",
      "#######################################################\n",
      "########## 10 random samples for items feats ##########\n",
      "#######################################################\n",
      "               V1        V2        V3  rating_mean  rating_count\n",
      "ItemId                                                          \n",
      "7359.0  -1.097616  0.311634  1.084853    -1.036886     -0.212289\n",
      "13226.0 -1.243456  0.142537 -1.444804     1.535033     -0.212289\n",
      "13745.0  0.550642  0.106852  0.050374    -1.036886     -0.212289\n",
      "8012.0   0.310404  0.301980 -1.625942     0.000000     -0.597702\n",
      "14474.0 -1.463011 -0.031197  0.325746     0.000000     -0.597702\n",
      "16286.0  0.394461  0.068769  1.428283     0.000000     -0.597702\n",
      "16105.0 -0.087607  0.087627 -0.971540    -1.036886     -0.212289\n",
      "5528.0   0.775766  0.292533 -1.489826     1.213543     -0.212289\n",
      "1033.0   1.122335  0.223305  1.370695    -1.036886     -0.212289\n",
      "20758.0  0.763038  0.059190  0.717340     0.249073      0.943952\n"
     ]
    }
   ],
   "source": [
    "## pre-processing for users\n",
    "user_cont = [\"V1\", \"V2\", \"V3\", \"V4\", \"rating_mean\", \"rating_count\"]\n",
    "user_pd[user_cont] = StandardScaler().fit_transform(user_pd[user_cont])\n",
    "\n",
    "## pre-processing for item\n",
    "item_cont = [\"V1\", \"V2\", \"V3\", \"rating_mean\", \"rating_count\"]\n",
    "item_pd[item_cont] = StandardScaler().fit_transform(item_pd[item_cont])\n",
    "\n",
    "\n",
    "user_pd = user_pd.set_index('UserId', drop=True)\n",
    "item_pd = item_pd.set_index('ItemId', drop=True)\n",
    "\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for users feats ##########')\n",
    "print('#######################################################')\n",
    "print(user_pd.sample(10))\n",
    "\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for items feats ##########')\n",
    "print('#######################################################')\n",
    "print(item_pd.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF model\n",
    "only two embeddings for categorical features, UserId and ItemId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SideNCF(keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size, **kwargs):\n",
    "        super(SideNCF, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.itme_embedding = layers.Embedding(\n",
    "            num_items,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "\n",
    "        self.concatenate = layers.Concatenate()\n",
    "        self.dense1 = layers.Dense(100, name='fc-1', activation='relu')\n",
    "        self.dense2 = layers.Dense(50, name='fc-2', activation='relu')\n",
    "        self.dense3 = layers.Dense(1, name='fc-3', activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cont_feats = inputs[0]\n",
    "        cate_feats = inputs[1]\n",
    "\n",
    "        user_vector = self.user_embedding(cate_feats[:,0])\n",
    "        itme_vector = self.itme_embedding(cate_feats[:,1])\n",
    "\n",
    "        concatted_vec = self.concatenate([cont_feats, user_vector, itme_vector])\n",
    "        fc_1 = self.dense1(concatted_vec)\n",
    "        fc_2 = self.dense2(fc_1)\n",
    "        fc_3 = self.dense3(fc_2)\n",
    "        return fc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SideNCF(num_users=n_user, num_items=n_item, embedding_size=50)\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "    keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3), \n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the continuous features and categorical features for user and item, respectively\n",
    "cate_feats = [\"UserId\", \"ItemId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cont = [\"V1\", \"V2\", \"V3\", 'rating_mean', 'rating_count']\n",
    "user_cont = [\"V1\", \"V2\", \"V3\", \"V4\", 'rating_mean', 'rating_count']\n",
    "\n",
    "train_cont_feats = np.hstack((user_pd.loc[train_pairs[:,0]][user_cont], item_pd.loc[train_pairs[:,1]][item_cont]))\n",
    "train_cate_feats = train_pairs.copy()\n",
    "\n",
    "test_cont_feats = np.hstack((user_pd.loc[test_pairs[:,0]][user_cont], item_pd.loc[test_pairs[:,1]][item_cont]))\n",
    "test_cate_feats = test_pairs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "504/504 [==============================] - 15s 28ms/step - loss: 2.0800 - mae: 0.9315 - rmse: 1.4208 - val_loss: 2.0260 - val_mae: 0.8210 - val_rmse: 1.4093\n",
      "Epoch 2/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.4943 - mae: 0.7457 - rmse: 1.2064 - val_loss: 2.0753 - val_mae: 0.8029 - val_rmse: 1.4270\n",
      "Epoch 3/50\n",
      "504/504 [==============================] - 13s 27ms/step - loss: 1.5303 - mae: 0.7380 - rmse: 1.2185 - val_loss: 1.4416 - val_mae: 0.7158 - val_rmse: 1.1824\n",
      "Epoch 4/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.4227 - mae: 0.7151 - rmse: 1.1727 - val_loss: 1.4545 - val_mae: 0.7325 - val_rmse: 1.1872\n",
      "Epoch 5/50\n",
      "504/504 [==============================] - 13s 27ms/step - loss: 1.4138 - mae: 0.7113 - rmse: 1.1676 - val_loss: 1.4504 - val_mae: 0.7331 - val_rmse: 1.1832\n",
      "Epoch 6/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 1.4237 - mae: 0.7056 - rmse: 1.1692 - val_loss: 1.4501 - val_mae: 0.7349 - val_rmse: 1.1803\n",
      "Epoch 7/50\n",
      "504/504 [==============================] - 12s 24ms/step - loss: 1.4161 - mae: 0.7051 - rmse: 1.1647 - val_loss: 1.4946 - val_mae: 0.7259 - val_rmse: 1.1981\n",
      "Epoch 8/50\n",
      "504/504 [==============================] - 12s 25ms/step - loss: 1.4050 - mae: 0.6992 - rmse: 1.1565 - val_loss: 1.4425 - val_mae: 0.7190 - val_rmse: 1.1733\n",
      "Epoch 9/50\n",
      "504/504 [==============================] - 13s 25ms/step - loss: 1.4063 - mae: 0.6952 - rmse: 1.1524 - val_loss: 1.4801 - val_mae: 0.7495 - val_rmse: 1.1860\n",
      "Epoch 10/50\n",
      "504/504 [==============================] - 12s 25ms/step - loss: 1.4037 - mae: 0.6932 - rmse: 1.1482 - val_loss: 1.5974 - val_mae: 0.7649 - val_rmse: 1.2294\n",
      "Epoch 11/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 1.4101 - mae: 0.6925 - rmse: 1.1463 - val_loss: 1.5273 - val_mae: 0.7303 - val_rmse: 1.1961\n",
      "Epoch 12/50\n",
      "504/504 [==============================] - 13s 25ms/step - loss: 1.4129 - mae: 0.6853 - rmse: 1.1407 - val_loss: 1.5042 - val_mae: 0.7208 - val_rmse: 1.1779\n",
      "Epoch 13/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 1.4126 - mae: 0.6769 - rmse: 1.1308 - val_loss: 1.5183 - val_mae: 0.7078 - val_rmse: 1.1735\n",
      "Epoch 14/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 1.4184 - mae: 0.6678 - rmse: 1.1226 - val_loss: 1.5652 - val_mae: 0.7030 - val_rmse: 1.1852\n",
      "Epoch 15/50\n",
      "211/504 [===========>..................] - ETA: 7s - loss: 1.3208 - mae: 0.6279 - rmse: 1.0751"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping( \n",
    "    monitor='val_rmse', min_delta=0, patience=5, verbose=1, \n",
    "    mode='auto', baseline=None, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    x=[train_cont_feats, train_cate_feats],\n",
    "    y=train_ratings,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_split=.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.        0.        ... 0.        0.        4.5314155]\n",
      "rmse: SideNCF: 0.854\n"
     ]
    }
   ],
   "source": [
    "pred_rating = model.predict([train_cont_feats, train_cate_feats]).flatten()\n",
    "pred_rating = adjustment.adjust(pred_rating)\n",
    "print(pred_rating)\n",
    "print('rmse: SideNCF: %.3f' %rmse(train_ratings, pred_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([test_cont_feats, test_cate_feats]).flatten()\n",
    "pred = adjustment.adjust(pred)\n",
    "sub[\"rating\"] = pred\n",
    "save_csv(sub, \"../predict\", \"NCF\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5129930097a52138fdc5ab816b09a2f27e944ad02f83c97d5e0e22f93f3b8c3c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
