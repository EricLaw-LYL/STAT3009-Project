{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from developed_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def mahalanobis_method(df):\n",
    "    #M-Distance\n",
    "    x_minus_mu = df - np.mean(df)\n",
    "    cov = np.cov(df.values.T)                           #Covariance\n",
    "    inv_covmat = sp.linalg.inv(cov)                     #Inverse covariance\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat) \n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    md = np.sqrt(mahal.diagonal())\n",
    "    \n",
    "    #Flag as outlier\n",
    "    outlier = []\n",
    "    #Cut-off point\n",
    "    C = np.sqrt(chi2.ppf((1-0.001), df=df.shape[1]))    #degrees of freedom = number of variables\n",
    "    for index, value in enumerate(md):\n",
    "        if value > C:\n",
    "            outlier.append(index)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier, md\n",
    "\n",
    "# save the predicted ratings to csv file\n",
    "def save_csv(df, folder_path, method):\n",
    "    nowTime = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    fileName = \"{folder_path}/{method}_{nowTime}.csv\".format(folder_path = folder_path, method = method, nowTime = nowTime)\n",
    "    df.to_csv(fileName, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_rating = pd.read_csv(\"../data/train_rating.csv\")\n",
    "test_pair = pd.read_csv(\"../data/test_pair.csv\")\n",
    "\n",
    "item_feat = pd.read_csv(\"../data/item_feats.csv\")\n",
    "user_feat = pd.read_csv(\"../data/user_feats.csv\")\n",
    "\n",
    "sub = pd.read_csv('../predict/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserID\n",
    "le_user = preprocessing.LabelEncoder()\n",
    "le_user.fit(np.append(np.append(train_rating['UserId'], test_pair[\"UserId\"]), user_feat[\"UserId\"]))\n",
    "\n",
    "user_feat['UserId'] = le_user.transform(user_feat[\"UserId\"])\n",
    "test_pair[\"UserId\"] = le_user.transform(test_pair[\"UserId\"])\n",
    "train_rating['UserId'] = le_user.transform(train_rating[\"UserId\"])\n",
    "\n",
    "# ItemID\n",
    "le_item = preprocessing.LabelEncoder()\n",
    "le_item.fit(np.append(np.append(train_rating['ItemId'], test_pair[\"ItemId\"]), item_feat[\"ItemId\"]))\n",
    "\n",
    "item_feat['ItemId'] = le_item.transform(item_feat[\"ItemId\"])\n",
    "test_pair[\"ItemId\"] = le_item.transform(test_pair[\"ItemId\"])\n",
    "train_rating['ItemId'] = le_item.transform(train_rating[\"ItemId\"])\n",
    "\n",
    "#Inf value\n",
    "user_feat.loc[np.isinf(user_feat['V1']),'V1']=-3\n",
    "item_feat.loc[np.isinf(item_feat['V2']),'V2']=2\n",
    "\n",
    "# Missing data\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(user_feat['V1'].values.reshape(-1, 1))\n",
    "user_feat['V1'] = imp_mean.transform(user_feat['V1'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tran_pair, train_rating\n",
    "train_pairs = train_rating[['UserId', 'ItemId']].values\n",
    "train_ratings = train_rating['rating'].values\n",
    "train_pair=train_rating.drop(columns='rating')\n",
    "\n",
    "# test_pair\n",
    "test_pairs = test_pair[['UserId', 'ItemId']].values\n",
    "\n",
    "# number of users and items\n",
    "# n_user, n_item = len(le_item.classes_), len(le_item.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user, n_item = max(train_pairs[:,0].max(), test_pairs[:,0].max())+1, max(train_pairs[:,1].max(), test_pairs[:,1].max())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minmium and maximum: [0.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "class min_max_adj:\n",
    "    def __init__(self, train_rating):\n",
    "        self.min = np.min(train_rating)\n",
    "        self.max = np.max(train_rating)\n",
    "        self.true_rating = train_rating\n",
    "    \n",
    "    def adjust(self, pred_rating):\n",
    "        pred_rating_adjusted = pred_rating.copy()\n",
    "        pred_rating_adjusted[pred_rating > self.max] = self.max\n",
    "        pred_rating_adjusted[pred_rating < self.min] = self.min\n",
    "        return pred_rating_adjusted\n",
    "\n",
    "    def rmse(self, pred_rating):\n",
    "        return np.sqrt(np.mean((pred_rating - self.true_rating)**2))\n",
    "\n",
    "adjustment = min_max_adj(train_rating[\"rating\"])\n",
    "print(\"Minmium and maximum:\", [adjustment.min, adjustment.max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "########## 10 random samples for users feats ##########\n",
      "#######################################################\n",
      "        UserId        V1      V2      V3     V4  rating_mean  rating_count\n",
      "20664  14405.0  2.766651  4277.0  1022.0  121.0     2.211891           0.0\n",
      "7429    1893.0  3.423729  4611.0   883.0  154.0     2.211891           0.0\n",
      "19990   9333.0  3.423729  1530.0   273.0  228.0     2.211891           0.0\n",
      "13378  18201.0  3.807812  1410.0   262.0  330.0     2.211891           0.0\n",
      "17075   3992.0  3.423729    64.0   396.0   86.0     2.211891           0.0\n",
      "26454  23129.0  3.423729  5624.0   262.0  330.0     2.211891           0.0\n",
      "15249  18821.0  3.320486  5608.0   588.0  121.0     2.211891           0.0\n",
      "12120   1998.0  3.230220  1030.0   123.0  325.0     2.211891           0.0\n",
      "12647  15880.0  3.423729  6755.0   844.0  330.0     2.211891           0.0\n",
      "6482   14257.0  3.423729   542.0  1521.0   22.0     2.211891           0.0\n",
      "#######################################################\n",
      "########## 10 random samples for items feats ##########\n",
      "#######################################################\n",
      "        ItemId       V1        V2      V3  rating_mean  rating_count\n",
      "20410  19651.0   8291.0  7.536327   978.0     3.500000           1.0\n",
      "22078  13991.0  12652.0  7.622576  2181.0     0.000000           1.0\n",
      "19117  22343.0  11037.0  7.565608  2974.0     0.000000           1.0\n",
      "4353   18711.0   3873.0  7.509552  1998.0     5.000000           1.0\n",
      "15072   9717.0  12603.0  7.558403  2001.0     1.612626           0.0\n",
      "15407  19730.0    727.0  7.607903   327.0     0.000000           1.0\n",
      "14073   7526.0  11691.0  7.699444  1059.0     4.750000           2.0\n",
      "9959    7431.0   5192.0  7.549630  1294.0     2.500000           3.0\n",
      "23531   4450.0   2651.0  7.680027  2655.0     2.250000           2.0\n",
      "12615  25507.0   8997.0  7.537014  3112.0     0.000000           1.0\n"
     ]
    }
   ],
   "source": [
    "## generate cont feats for users\n",
    "user_pd = pd.merge(left=train_rating.groupby('UserId')['rating'].mean(), \n",
    "\t\t\t\t   right=train_rating.groupby('UserId')['rating'].count(), on='UserId')\n",
    "user_pd.columns = ['rating_mean', 'rating_count']\n",
    "user_pd = pd.merge(left = user_feat, right = user_pd, on = \"UserId\", how = \"outer\") # using outer join\n",
    "\n",
    "## handle missing data\n",
    "# if the user has no rating record, set rating_count = 0\n",
    "user_pd.fillna(value = {\"rating_count\": 0}, inplace = True)\n",
    "# if the rating_mean is missing, then use global mean\n",
    "imp_mean.fit(user_pd)\n",
    "user_pd = pd.DataFrame(imp_mean.transform(user_pd), columns = user_pd.columns)\n",
    "\n",
    "## generate cont feats for items\n",
    "item_rating_pd = pd.merge(left=train_rating.groupby('ItemId')['rating'].mean(), \n",
    "\t\t\t\t\t\t  right=train_rating.groupby('ItemId')['rating'].count(), on='ItemId')\n",
    "item_rating_pd.columns\t= ['rating_mean', 'rating_count']\n",
    "item_pd = pd.merge(left=item_feat, right=item_rating_pd, on='ItemId', how = \"outer\") # using outer join\n",
    "\n",
    "## handle missing data\n",
    "# if the item has no rating record, set rating_count = 0\n",
    "item_pd.fillna(value = {\"rating_count\": 0}, inplace = True)\n",
    "# if the rating_mean is missing, then use global mean\n",
    "imp_mean.fit(item_pd)\n",
    "item_pd = pd.DataFrame(imp_mean.transform(item_pd), columns = item_pd.columns)\n",
    "\n",
    "\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for users feats ##########')\n",
    "print('#######################################################')\n",
    "\n",
    "print(user_pd.sample(10))\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for items feats ##########')\n",
    "print('#######################################################')\n",
    "\n",
    "print(item_pd.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "########## 10 random samples for users feats ##########\n",
      "#######################################################\n",
      "               V1        V2        V3        V4   rating_mean  rating_count\n",
      "UserId                                                                     \n",
      "19188.0  0.567255  1.115762  0.477135 -1.068355 -5.564071e-16     -0.067335\n",
      "9467.0  -0.174131 -0.895074 -1.019990  0.762978 -5.564071e-16     -0.067335\n",
      "21423.0  0.000000 -0.747188  1.379421 -1.068355 -1.675014e+00      0.113023\n",
      "2688.0   0.000000 -1.220848 -1.672755 -1.637908 -5.564071e-16     -0.067335\n",
      "13564.0  0.467600 -0.895074 -1.019990  0.762978 -5.564071e-16     -0.067335\n",
      "14437.0 -0.051990  1.278014  0.289995 -1.734294 -5.564071e-16     -0.067335\n",
      "26377.0 -0.549315  0.015907 -0.079831  0.105801 -5.564071e-16     -0.067335\n",
      "22673.0  0.000000  0.937030 -1.216042  0.762978  2.240353e+00      0.022844\n",
      "1254.0   0.000000 -0.947469 -1.040041  0.719167 -5.564071e-16     -0.067335\n",
      "25535.0  0.000000  1.084072  0.078348  0.762978 -5.564071e-16     -0.067335\n",
      "#######################################################\n",
      "########## 10 random samples for items feats ##########\n",
      "#######################################################\n",
      "               V1        V2        V3  rating_mean  rating_count\n",
      "ItemId                                                          \n",
      "22012.0  0.580606  0.206442 -1.428051     0.000000     -0.597702\n",
      "23212.0 -0.546340  0.164626 -1.597672     0.249073     -0.212289\n",
      "11259.0  0.816601  0.319440  0.855550     0.000000     -0.597702\n",
      "24974.0 -1.249554  0.112657  0.714199     0.000000     -0.597702\n",
      "24292.0  1.257304  0.106753  1.256568     0.000000     -0.597702\n",
      "9089.0  -0.302390  0.124020  0.453486    -1.036886     -0.212289\n",
      "5925.0   0.763038  0.043889 -0.128670    -0.286743      0.558538\n",
      "10738.0  1.707817  0.200392  1.255521     0.892053     -0.212289\n",
      "12538.0  0.437152 -0.042220 -0.412419    -1.036886     -0.212289\n",
      "5535.0   1.645768  0.174883 -1.417580    -1.036886     -0.212289\n"
     ]
    }
   ],
   "source": [
    "## pre-processing for users\n",
    "user_cont = [\"V1\", \"V2\", \"V3\", \"V4\", \"rating_mean\", \"rating_count\"]\n",
    "user_pd[user_cont] = preprocessing.StandardScaler().fit_transform(user_pd[user_cont])\n",
    "\n",
    "## pre-processing for item\n",
    "item_cont = [\"V1\", \"V2\", \"V3\", \"rating_mean\", \"rating_count\"]\n",
    "item_pd[item_cont] = preprocessing.StandardScaler().fit_transform(item_pd[item_cont])\n",
    "\n",
    "\n",
    "user_pd = user_pd.set_index('UserId', drop=True)\n",
    "item_pd = item_pd.set_index('ItemId', drop=True)\n",
    "\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for users feats ##########')\n",
    "print('#######################################################')\n",
    "print(user_pd.sample(10))\n",
    "\n",
    "print('#######################################################')\n",
    "print('########## 10 random samples for items feats ##########')\n",
    "print('#######################################################')\n",
    "print(item_pd.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, Dropout, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import SVG\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SideNCF(keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size, **kwargs):\n",
    "        super(SideNCF, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.itme_embedding = layers.Embedding(\n",
    "            num_items,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "\n",
    "        self.concatenate = layers.Concatenate()\n",
    "        self.dense1 = layers.Dense(100, name='fc-1', activation='relu')\n",
    "        self.dense2 = layers.Dense(50, name='fc-2', activation='relu')\n",
    "        self.dense3 = layers.Dense(1, name='fc-3', activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cont_feats = inputs[0]\n",
    "        cate_feats = inputs[1]\n",
    "\n",
    "        user_vector = self.user_embedding(cate_feats[:,0])\n",
    "        itme_vector = self.itme_embedding(cate_feats[:,1])\n",
    "\n",
    "        concatted_vec = self.concatenate([cont_feats, user_vector, itme_vector])\n",
    "        fc_1 = self.dense1(concatted_vec)\n",
    "        fc_2 = self.dense2(fc_1)\n",
    "        fc_3 = self.dense3(fc_2)\n",
    "        return fc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SideNCF(num_users=n_user, num_items=n_item, embedding_size=50)\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "    keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3), \n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the continuous features and categorical features for user and item, respectively\n",
    "item_cont, item_cate = [\"V1\", \"V2\", \"V3\", 'rating_mean', 'rating_count'], ['ItemId']\n",
    "user_cont, user_cate = [\"V1\", \"V2\", \"V3\", \"V4\", 'rating_mean', 'rating_count'], ['UserId']\n",
    "\n",
    "train_cont_feats = np.hstack((user_pd.loc[train_pairs[:,0]][user_cont], item_pd.loc[train_pairs[:,1]][item_cont]))\n",
    "train_cate_feats = train_pairs.copy()\n",
    "\n",
    "test_cont_feats = np.hstack((user_pd.loc[test_pairs[:,0]][user_cont], item_pd.loc[test_pairs[:,1]][item_cont]))\n",
    "test_cate_feats = test_pairs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "504/504 [==============================] - 16s 29ms/step - loss: 2.6807 - mae: 0.9918 - rmse: 1.6188 - val_loss: 1.4861 - val_mae: 0.7722 - val_rmse: 1.2042\n",
      "Epoch 2/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.4736 - mae: 0.7457 - rmse: 1.1997 - val_loss: 1.4725 - val_mae: 0.7373 - val_rmse: 1.2001\n",
      "Epoch 3/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.4612 - mae: 0.7337 - rmse: 1.1947 - val_loss: 1.5574 - val_mae: 0.7438 - val_rmse: 1.2361\n",
      "Epoch 4/50\n",
      "504/504 [==============================] - 14s 27ms/step - loss: 1.4104 - mae: 0.7178 - rmse: 1.1731 - val_loss: 1.4271 - val_mae: 0.7299 - val_rmse: 1.1799\n",
      "Epoch 5/50\n",
      "504/504 [==============================] - 15s 30ms/step - loss: 1.4113 - mae: 0.7120 - rmse: 1.1705 - val_loss: 1.4330 - val_mae: 0.7188 - val_rmse: 1.1784\n",
      "Epoch 6/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.4188 - mae: 0.7130 - rmse: 1.1716 - val_loss: 1.5547 - val_mae: 0.7535 - val_rmse: 1.2284\n",
      "Epoch 7/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.3983 - mae: 0.7040 - rmse: 1.1628 - val_loss: 1.4836 - val_mae: 0.7453 - val_rmse: 1.1993\n",
      "Epoch 8/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.3947 - mae: 0.7041 - rmse: 1.1602 - val_loss: 1.4837 - val_mae: 0.7320 - val_rmse: 1.1982\n",
      "Epoch 9/50\n",
      "504/504 [==============================] - 15s 30ms/step - loss: 1.3868 - mae: 0.6998 - rmse: 1.1560 - val_loss: 1.4380 - val_mae: 0.7095 - val_rmse: 1.1765\n",
      "Epoch 10/50\n",
      "504/504 [==============================] - 15s 29ms/step - loss: 1.3856 - mae: 0.6956 - rmse: 1.1517 - val_loss: 1.4443 - val_mae: 0.7238 - val_rmse: 1.1759\n",
      "Epoch 11/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.3908 - mae: 0.6951 - rmse: 1.1521 - val_loss: 1.4514 - val_mae: 0.7045 - val_rmse: 1.1777\n",
      "Epoch 12/50\n",
      "504/504 [==============================] - 14s 29ms/step - loss: 1.3875 - mae: 0.6885 - rmse: 1.1471 - val_loss: 1.4772 - val_mae: 0.7368 - val_rmse: 1.1851\n",
      "Epoch 13/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 1.3921 - mae: 0.6892 - rmse: 1.1464 - val_loss: 1.4649 - val_mae: 0.7382 - val_rmse: 1.1785\n",
      "Epoch 14/50\n",
      "504/504 [==============================] - 16s 32ms/step - loss: 1.3878 - mae: 0.6843 - rmse: 1.1400 - val_loss: 1.4842 - val_mae: 0.7124 - val_rmse: 1.1807\n",
      "Epoch 15/50\n",
      "504/504 [==============================] - 15s 30ms/step - loss: 1.3964 - mae: 0.6847 - rmse: 1.1397 - val_loss: 1.4883 - val_mae: 0.7050 - val_rmse: 1.1789\n",
      "Epoch 16/50\n",
      "504/504 [==============================] - 17s 33ms/step - loss: 1.3977 - mae: 0.6762 - rmse: 1.1330 - val_loss: 1.4996 - val_mae: 0.7023 - val_rmse: 1.1773\n",
      "Epoch 17/50\n",
      "504/504 [==============================] - 22s 43ms/step - loss: 1.3926 - mae: 0.6655 - rmse: 1.1216 - val_loss: 1.5243 - val_mae: 0.7049 - val_rmse: 1.1772\n",
      "Epoch 18/50\n",
      "504/504 [==============================] - 19s 38ms/step - loss: 1.3852 - mae: 0.6516 - rmse: 1.1093 - val_loss: 1.5778 - val_mae: 0.7214 - val_rmse: 1.1895\n",
      "Epoch 19/50\n",
      "504/504 [==============================] - 19s 38ms/step - loss: 1.3831 - mae: 0.6418 - rmse: 1.0979 - val_loss: 1.6057 - val_mae: 0.7098 - val_rmse: 1.1934\n",
      "Epoch 20/50\n",
      "504/504 [==============================] - 20s 40ms/step - loss: 1.3552 - mae: 0.6204 - rmse: 1.0749 - val_loss: 1.6572 - val_mae: 0.7228 - val_rmse: 1.2045\n",
      "Epoch 21/50\n",
      "504/504 [==============================] - 19s 38ms/step - loss: 1.3199 - mae: 0.5984 - rmse: 1.0516 - val_loss: 1.7791 - val_mae: 0.7479 - val_rmse: 1.2454\n",
      "Epoch 22/50\n",
      "504/504 [==============================] - 20s 40ms/step - loss: 1.2914 - mae: 0.5809 - rmse: 1.0284 - val_loss: 1.7638 - val_mae: 0.6994 - val_rmse: 1.2323\n",
      "Epoch 23/50\n",
      "504/504 [==============================] - 21s 42ms/step - loss: 1.2460 - mae: 0.5547 - rmse: 1.0003 - val_loss: 1.8257 - val_mae: 0.7068 - val_rmse: 1.2549\n",
      "Epoch 24/50\n",
      "504/504 [==============================] - 19s 37ms/step - loss: 1.2097 - mae: 0.5340 - rmse: 0.9780 - val_loss: 1.8436 - val_mae: 0.7019 - val_rmse: 1.2555\n",
      "Epoch 25/50\n",
      "504/504 [==============================] - 21s 42ms/step - loss: 1.1722 - mae: 0.5134 - rmse: 0.9545 - val_loss: 1.8987 - val_mae: 0.7034 - val_rmse: 1.2739\n",
      "Epoch 26/50\n",
      "504/504 [==============================] - 20s 39ms/step - loss: 1.1376 - mae: 0.4948 - rmse: 0.9339 - val_loss: 1.9618 - val_mae: 0.7230 - val_rmse: 1.2960\n",
      "Epoch 27/50\n",
      "504/504 [==============================] - 18s 36ms/step - loss: 1.1083 - mae: 0.4771 - rmse: 0.9142 - val_loss: 2.0466 - val_mae: 0.7314 - val_rmse: 1.3230\n",
      "Epoch 28/50\n",
      "504/504 [==============================] - 18s 36ms/step - loss: 1.0891 - mae: 0.4643 - rmse: 0.8990 - val_loss: 2.0944 - val_mae: 0.7353 - val_rmse: 1.3391\n",
      "Epoch 29/50\n",
      "504/504 [==============================] - 19s 37ms/step - loss: 1.0523 - mae: 0.4457 - rmse: 0.8778 - val_loss: 2.0926 - val_mae: 0.7140 - val_rmse: 1.3365\n",
      "Epoch 30/50\n",
      "504/504 [==============================] - 18s 35ms/step - loss: 1.0244 - mae: 0.4300 - rmse: 0.8585 - val_loss: 2.1205 - val_mae: 0.7142 - val_rmse: 1.3457\n",
      "Epoch 31/50\n",
      "504/504 [==============================] - 18s 35ms/step - loss: 1.0108 - mae: 0.4207 - rmse: 0.8468 - val_loss: 2.1519 - val_mae: 0.7247 - val_rmse: 1.3551\n",
      "Epoch 32/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 0.9814 - mae: 0.4079 - rmse: 0.8284 - val_loss: 2.2869 - val_mae: 0.7413 - val_rmse: 1.4032\n",
      "Epoch 33/50\n",
      "504/504 [==============================] - 15s 30ms/step - loss: 0.9593 - mae: 0.3948 - rmse: 0.8133 - val_loss: 2.3276 - val_mae: 0.7636 - val_rmse: 1.4148\n",
      "Epoch 34/50\n",
      "504/504 [==============================] - 15s 31ms/step - loss: 0.9432 - mae: 0.3853 - rmse: 0.8015 - val_loss: 2.2333 - val_mae: 0.7212 - val_rmse: 1.3824\n",
      "Epoch 35/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 0.9168 - mae: 0.3766 - rmse: 0.7864 - val_loss: 2.3207 - val_mae: 0.7323 - val_rmse: 1.4134\n",
      "Epoch 36/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 0.9203 - mae: 0.3732 - rmse: 0.7875 - val_loss: 2.2575 - val_mae: 0.7221 - val_rmse: 1.3893\n",
      "Epoch 37/50\n",
      "504/504 [==============================] - 14s 28ms/step - loss: 0.9094 - mae: 0.3670 - rmse: 0.7791 - val_loss: 2.2781 - val_mae: 0.7246 - val_rmse: 1.3964\n",
      "Epoch 38/50\n",
      "504/504 [==============================] - 14s 27ms/step - loss: 0.8832 - mae: 0.3574 - rmse: 0.7646 - val_loss: 2.4149 - val_mae: 0.7557 - val_rmse: 1.4439\n",
      "Epoch 39/50\n",
      "504/504 [==============================] - 14s 27ms/step - loss: 0.8815 - mae: 0.3550 - rmse: 0.7611 - val_loss: 2.3330 - val_mae: 0.7324 - val_rmse: 1.4169\n",
      "Epoch 40/50\n",
      "504/504 [==============================] - 13s 25ms/step - loss: 0.8707 - mae: 0.3484 - rmse: 0.7557 - val_loss: 2.3630 - val_mae: 0.7371 - val_rmse: 1.4273\n",
      "Epoch 41/50\n",
      "504/504 [==============================] - 13s 25ms/step - loss: 0.8535 - mae: 0.3409 - rmse: 0.7450 - val_loss: 2.3159 - val_mae: 0.7217 - val_rmse: 1.4130\n",
      "Epoch 42/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 0.8584 - mae: 0.3448 - rmse: 0.7482 - val_loss: 2.4428 - val_mae: 0.7534 - val_rmse: 1.4502\n",
      "Epoch 43/50\n",
      "504/504 [==============================] - 12s 24ms/step - loss: 0.8564 - mae: 0.3393 - rmse: 0.7425 - val_loss: 2.3302 - val_mae: 0.7230 - val_rmse: 1.4138\n",
      "Epoch 44/50\n",
      "504/504 [==============================] - 14s 27ms/step - loss: 0.8463 - mae: 0.3346 - rmse: 0.7372 - val_loss: 2.3118 - val_mae: 0.7189 - val_rmse: 1.4086\n",
      "Epoch 45/50\n",
      "504/504 [==============================] - 13s 26ms/step - loss: 0.8354 - mae: 0.3302 - rmse: 0.7310 - val_loss: 2.3207 - val_mae: 0.7221 - val_rmse: 1.4139\n",
      "Epoch 46/50\n",
      "504/504 [==============================] - 15s 30ms/step - loss: 0.8185 - mae: 0.3258 - rmse: 0.7248 - val_loss: 2.3220 - val_mae: 0.7099 - val_rmse: 1.4135\n",
      "Epoch 47/50\n",
      "504/504 [==============================] - 13s 27ms/step - loss: 0.8287 - mae: 0.3262 - rmse: 0.7277 - val_loss: 2.3290 - val_mae: 0.7089 - val_rmse: 1.4151\n",
      "Epoch 48/50\n",
      "504/504 [==============================] - 13s 27ms/step - loss: 0.8215 - mae: 0.3206 - rmse: 0.7190 - val_loss: 2.3925 - val_mae: 0.7294 - val_rmse: 1.4377\n",
      "Epoch 49/50\n",
      "504/504 [==============================] - 14s 27ms/step - loss: 0.8114 - mae: 0.3189 - rmse: 0.7157 - val_loss: 2.3565 - val_mae: 0.7236 - val_rmse: 1.4260\n",
      "Epoch 50/50\n",
      "504/504 [==============================] - 14s 27ms/step - loss: 0.8034 - mae: 0.3136 - rmse: 0.7110 - val_loss: 2.3714 - val_mae: 0.7254 - val_rmse: 1.4327\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping( \n",
    "    monitor='val_rmse', min_delta=0, patience=5, verbose=1, \n",
    "    mode='auto', baseline=None, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    x=[train_cont_feats, train_cate_feats],\n",
    "    y=train_ratings,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_split=.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.        0.        ... 0.        0.        4.5314155]\n",
      "rmse: SideNCF: 0.854\n"
     ]
    }
   ],
   "source": [
    "pred_rating = model.predict([train_cont_feats, train_cate_feats]).flatten()\n",
    "pred_rating = adjustment.adjust(pred_rating)\n",
    "print(pred_rating)\n",
    "print('rmse: SideNCF: %.3f' %rmse(train_ratings, pred_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predicted ratings to csv file\n",
    "def save_csv(df, folder_path, method):\n",
    "    nowTime = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    fileName = \"{folder_path}/{method}_{nowTime}.csv\".format(folder_path = folder_path, method = method, nowTime = nowTime)\n",
    "    df.to_csv(fileName, index = False)\n",
    "\n",
    "sub = pd.read_csv('../predict/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([test_cont_feats, test_cate_feats]).flatten()\n",
    "pred = adjustment.adjust(pred)\n",
    "sub[\"rating\"] = pred\n",
    "save_csv(sub, \"../predict\", \"NCF\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5129930097a52138fdc5ab816b09a2f27e944ad02f83c97d5e0e22f93f3b8c3c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
