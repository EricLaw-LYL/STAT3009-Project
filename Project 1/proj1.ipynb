{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT3009 Project 1\n",
    "- LAW Yiu Leung Eric 1155149315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the developed methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from developed_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the raw data\n",
    "- check the `user_id` and `item_id`: mapping `item_id` to a continuous sequence based on `sklean.preprocessing`\n",
    "- use `sklearn.model_selection.train_test_split` to generate `train` and `test` dataset\n",
    "- create `train_pair`,`train_rating`, `test_pair`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "dtest_submit = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "## mapping \n",
    "from sklearn import preprocessing\n",
    "le_user = preprocessing.LabelEncoder()\n",
    "le_user.fit(np.append(df['user_id'], dtest_submit[\"user_id\"]))\n",
    "df['user_id'] = le_user.transform(df[\"user_id\"])\n",
    "dtest_submit[\"user_id\"] = le_user.transform(dtest_submit[\"user_id\"])\n",
    "\n",
    "le_item = preprocessing.LabelEncoder()\n",
    "le_item.fit(np.append(df['item_id'], dtest_submit[\"item_id\"]))\n",
    "df[\"item_id\"] = le_item.transform(df[\"item_id\"])\n",
    "dtest_submit[\"item_id\"] = le_item.transform(dtest_submit[\"item_id\"])\n",
    "\n",
    "## generate train / test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtrain, dtest = train_test_split(df, test_size=0.33, random_state=42)\n",
    "\n",
    "## save real ratings for test set for evaluation.\n",
    "test_rating = np.array(dtest['rating'])\n",
    "\n",
    "## remove the ratings in the test set to simulate prediction\n",
    "dtest = dtest.drop(columns='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train_pair, train_rating and test_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pair, train_rating\n",
    "train_pair = dtrain[['user_id', 'item_id']].values\n",
    "train_rating = dtrain['rating'].values\n",
    "\n",
    "# test_pair\n",
    "test_pair = dtest[['user_id', 'item_id']].values\n",
    "n_user, n_item = max(train_pair[:,0].max(), test_pair[:,0].max())+1, max(train_pair[:,1].max(), test_pair[:,1].max())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impelement deep learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "We embed both users and movies in to 50-dimensional vectors.\n",
    "\n",
    "The model computes a match score between user and movie embeddings via a dot product, and adds a per-movie and per-user bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFactorNet(keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size, **kwargs):\n",
    "        super(LFactorNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "        self.glb_bias = tf.Variable(0., trainable=True) \n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_items,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.movie_bias = layers.Embedding(num_items, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
    "        # Add all the components (including bias)\n",
    "        x = dot_user_movie + user_bias + movie_bias + self.glb_bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "129/129 [==============================] - 2s 4ms/step - loss: 46.4377 - mae: 5.9697 - rmse: 6.7121 - val_loss: 43.8266 - val_mae: 5.7573 - val_rmse: 6.5201\n",
      "Epoch 2/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 33.6719 - mae: 4.8097 - rmse: 5.6781 - val_loss: 40.0347 - val_mae: 5.4005 - val_rmse: 6.2015\n",
      "Epoch 3/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 21.8221 - mae: 3.4677 - rmse: 4.4763 - val_loss: 33.2343 - val_mae: 4.7102 - val_rmse: 5.5875\n",
      "Epoch 4/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 13.6518 - mae: 2.4192 - rmse: 3.3856 - val_loss: 26.7687 - val_mae: 3.9902 - val_rmse: 4.9420\n",
      "Epoch 5/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 11.6849 - mae: 2.2524 - rmse: 3.0519 - val_loss: 24.6538 - val_mae: 3.7524 - val_rmse: 4.7199\n",
      "Epoch 6/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 11.4125 - mae: 2.2476 - rmse: 3.0117 - val_loss: 24.0462 - val_mae: 3.6983 - val_rmse: 4.6623\n",
      "Epoch 7/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 11.2094 - mae: 2.2252 - rmse: 2.9894 - val_loss: 23.2594 - val_mae: 3.6218 - val_rmse: 4.5838\n",
      "Epoch 8/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.9646 - mae: 2.1976 - rmse: 2.9591 - val_loss: 22.4980 - val_mae: 3.5471 - val_rmse: 4.5074\n",
      "Epoch 9/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.7471 - mae: 2.1789 - rmse: 2.9339 - val_loss: 21.8063 - val_mae: 3.4820 - val_rmse: 4.4387\n",
      "Epoch 10/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.5423 - mae: 2.1608 - rmse: 2.9113 - val_loss: 20.9641 - val_mae: 3.3952 - val_rmse: 4.3506\n",
      "Epoch 11/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.3051 - mae: 2.1447 - rmse: 2.8818 - val_loss: 20.4251 - val_mae: 3.3484 - val_rmse: 4.2979\n",
      "Epoch 12/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.0965 - mae: 2.1163 - rmse: 2.8588 - val_loss: 19.2593 - val_mae: 3.2157 - val_rmse: 4.1659\n",
      "Epoch 13/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 9.8861 - mae: 2.1084 - rmse: 2.8330 - val_loss: 18.5756 - val_mae: 3.1452 - val_rmse: 4.0923\n",
      "Epoch 14/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 9.7417 - mae: 2.0920 - rmse: 2.8182 - val_loss: 17.9981 - val_mae: 3.0885 - val_rmse: 4.0304\n",
      "Epoch 15/200\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 9.5120 - mae: 2.0644 - rmse: 2.7897 - val_loss: 17.0065 - val_mae: 2.9720 - val_rmse: 3.9127\n",
      "Epoch 16/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 9.2996 - mae: 2.0447 - rmse: 2.7633 - val_loss: 16.3380 - val_mae: 2.8984 - val_rmse: 3.8357\n",
      "Epoch 17/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 9.0583 - mae: 2.0155 - rmse: 2.7323 - val_loss: 15.2189 - val_mae: 2.7588 - val_rmse: 3.6932\n",
      "Epoch 18/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.9148 - mae: 2.0103 - rmse: 2.7165 - val_loss: 14.7471 - val_mae: 2.7099 - val_rmse: 3.6390\n",
      "Epoch 19/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.7271 - mae: 1.9869 - rmse: 2.6931 - val_loss: 14.0146 - val_mae: 2.6211 - val_rmse: 3.5451\n",
      "Epoch 20/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.5189 - mae: 1.9669 - rmse: 2.6642 - val_loss: 13.2314 - val_mae: 2.5246 - val_rmse: 3.4406\n",
      "Epoch 21/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.3802 - mae: 1.9554 - rmse: 2.6482 - val_loss: 12.6048 - val_mae: 2.4490 - val_rmse: 3.3559\n",
      "Epoch 22/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.2180 - mae: 1.9332 - rmse: 2.6282 - val_loss: 11.9296 - val_mae: 2.3612 - val_rmse: 3.2601\n",
      "Epoch 23/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.0283 - mae: 1.9174 - rmse: 2.6005 - val_loss: 11.3420 - val_mae: 2.2875 - val_rmse: 3.1750\n",
      "Epoch 24/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.9085 - mae: 1.9003 - rmse: 2.5866 - val_loss: 10.6711 - val_mae: 2.2010 - val_rmse: 3.0736\n",
      "Epoch 25/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.7209 - mae: 1.8761 - rmse: 2.5575 - val_loss: 10.2562 - val_mae: 2.1510 - val_rmse: 3.0106\n",
      "Epoch 26/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.5836 - mae: 1.8633 - rmse: 2.5369 - val_loss: 10.1233 - val_mae: 2.1428 - val_rmse: 2.9983\n",
      "Epoch 27/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.4602 - mae: 1.8531 - rmse: 2.5205 - val_loss: 9.9357 - val_mae: 2.1278 - val_rmse: 2.9755\n",
      "Epoch 28/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.2959 - mae: 1.8244 - rmse: 2.4947 - val_loss: 9.3162 - val_mae: 2.0459 - val_rmse: 2.8714\n",
      "Epoch 29/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.1678 - mae: 1.8140 - rmse: 2.4757 - val_loss: 9.0530 - val_mae: 2.0162 - val_rmse: 2.8321\n",
      "Epoch 30/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.0488 - mae: 1.7954 - rmse: 2.4577 - val_loss: 8.6180 - val_mae: 1.9649 - val_rmse: 2.7563\n",
      "Epoch 31/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.9101 - mae: 1.7854 - rmse: 2.4349 - val_loss: 8.5559 - val_mae: 1.9599 - val_rmse: 2.7535\n",
      "Epoch 32/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.7679 - mae: 1.7677 - rmse: 2.4112 - val_loss: 8.3500 - val_mae: 1.9387 - val_rmse: 2.7212\n",
      "Epoch 33/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.6793 - mae: 1.7467 - rmse: 2.3972 - val_loss: 7.8577 - val_mae: 1.8789 - val_rmse: 2.6266\n",
      "Epoch 34/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.5633 - mae: 1.7328 - rmse: 2.3762 - val_loss: 8.0378 - val_mae: 1.9057 - val_rmse: 2.6712\n",
      "Epoch 35/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.4846 - mae: 1.7294 - rmse: 2.3636 - val_loss: 7.6879 - val_mae: 1.8646 - val_rmse: 2.6060\n",
      "Epoch 36/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.3965 - mae: 1.7119 - rmse: 2.3481 - val_loss: 7.5846 - val_mae: 1.8572 - val_rmse: 2.5904\n",
      "Epoch 37/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.2320 - mae: 1.6886 - rmse: 2.3159 - val_loss: 7.3067 - val_mae: 1.8240 - val_rmse: 2.5361\n",
      "Epoch 38/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.1630 - mae: 1.6835 - rmse: 2.3043 - val_loss: 7.1426 - val_mae: 1.8002 - val_rmse: 2.5073\n",
      "Epoch 39/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.0597 - mae: 1.6693 - rmse: 2.2840 - val_loss: 6.9507 - val_mae: 1.7780 - val_rmse: 2.4689\n",
      "Epoch 40/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.9581 - mae: 1.6475 - rmse: 2.2637 - val_loss: 6.8501 - val_mae: 1.7682 - val_rmse: 2.4501\n",
      "Epoch 41/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.8789 - mae: 1.6358 - rmse: 2.2496 - val_loss: 6.7868 - val_mae: 1.7574 - val_rmse: 2.4412\n",
      "Epoch 42/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.7936 - mae: 1.6248 - rmse: 2.2314 - val_loss: 6.8320 - val_mae: 1.7603 - val_rmse: 2.4525\n",
      "Epoch 43/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.6975 - mae: 1.6090 - rmse: 2.2121 - val_loss: 6.6509 - val_mae: 1.7374 - val_rmse: 2.4154\n",
      "Epoch 44/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.6283 - mae: 1.5991 - rmse: 2.1973 - val_loss: 6.8436 - val_mae: 1.7602 - val_rmse: 2.4653\n",
      "Epoch 45/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.5435 - mae: 1.5749 - rmse: 2.1809 - val_loss: 6.3465 - val_mae: 1.7027 - val_rmse: 2.3511\n",
      "Epoch 46/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.4414 - mae: 1.5696 - rmse: 2.1591 - val_loss: 6.2473 - val_mae: 1.6887 - val_rmse: 2.3334\n",
      "Epoch 47/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.3773 - mae: 1.5615 - rmse: 2.1460 - val_loss: 6.1846 - val_mae: 1.6826 - val_rmse: 2.3233\n",
      "Epoch 48/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.3352 - mae: 1.5556 - rmse: 2.1368 - val_loss: 6.1603 - val_mae: 1.6756 - val_rmse: 2.3203\n",
      "Epoch 49/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.2406 - mae: 1.5342 - rmse: 2.1159 - val_loss: 6.0082 - val_mae: 1.6622 - val_rmse: 2.2837\n",
      "Epoch 50/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.1679 - mae: 1.5252 - rmse: 2.0989 - val_loss: 6.0999 - val_mae: 1.6676 - val_rmse: 2.3132\n",
      "Epoch 51/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.0798 - mae: 1.5093 - rmse: 2.0791 - val_loss: 5.9902 - val_mae: 1.6523 - val_rmse: 2.2901\n",
      "Epoch 52/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.0439 - mae: 1.5008 - rmse: 2.0706 - val_loss: 5.8405 - val_mae: 1.6329 - val_rmse: 2.2517\n",
      "Epoch 53/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.9482 - mae: 1.4902 - rmse: 2.0486 - val_loss: 5.8859 - val_mae: 1.6292 - val_rmse: 2.2698\n",
      "Epoch 54/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.8904 - mae: 1.4732 - rmse: 2.0356 - val_loss: 5.7801 - val_mae: 1.6169 - val_rmse: 2.2457\n",
      "Epoch 55/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.8222 - mae: 1.4661 - rmse: 2.0206 - val_loss: 5.6591 - val_mae: 1.5983 - val_rmse: 2.2150\n",
      "Epoch 56/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.7519 - mae: 1.4557 - rmse: 2.0054 - val_loss: 5.6528 - val_mae: 1.5936 - val_rmse: 2.2186\n",
      "Epoch 57/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.7077 - mae: 1.4564 - rmse: 1.9962 - val_loss: 5.5327 - val_mae: 1.5781 - val_rmse: 2.1877\n",
      "Epoch 58/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.6328 - mae: 1.4362 - rmse: 1.9773 - val_loss: 5.5433 - val_mae: 1.5813 - val_rmse: 2.1960\n",
      "Epoch 59/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.5740 - mae: 1.4242 - rmse: 1.9643 - val_loss: 5.4668 - val_mae: 1.5718 - val_rmse: 2.1786\n",
      "Epoch 60/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.5169 - mae: 1.4214 - rmse: 1.9511 - val_loss: 5.5708 - val_mae: 1.5860 - val_rmse: 2.2076\n",
      "Epoch 61/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.4587 - mae: 1.4093 - rmse: 1.9376 - val_loss: 5.4004 - val_mae: 1.5618 - val_rmse: 2.1650\n",
      "Epoch 62/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.3930 - mae: 1.3956 - rmse: 1.9209 - val_loss: 5.4183 - val_mae: 1.5639 - val_rmse: 2.1741\n",
      "Epoch 63/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.3388 - mae: 1.3911 - rmse: 1.9076 - val_loss: 5.2382 - val_mae: 1.5419 - val_rmse: 2.1290\n",
      "Epoch 64/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.2583 - mae: 1.3730 - rmse: 1.8880 - val_loss: 5.2342 - val_mae: 1.5428 - val_rmse: 2.1303\n",
      "Epoch 65/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.2192 - mae: 1.3669 - rmse: 1.8788 - val_loss: 5.2443 - val_mae: 1.5434 - val_rmse: 2.1361\n",
      "Epoch 66/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.2083 - mae: 1.3682 - rmse: 1.8768 - val_loss: 5.1306 - val_mae: 1.5249 - val_rmse: 2.1084\n",
      "Epoch 67/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.1258 - mae: 1.3551 - rmse: 1.8551 - val_loss: 5.1532 - val_mae: 1.5300 - val_rmse: 2.1180\n",
      "Epoch 68/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.0785 - mae: 1.3388 - rmse: 1.8451 - val_loss: 4.9227 - val_mae: 1.4983 - val_rmse: 2.0576\n",
      "Epoch 69/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.0251 - mae: 1.3369 - rmse: 1.8319 - val_loss: 4.9358 - val_mae: 1.4943 - val_rmse: 2.0662\n",
      "Epoch 70/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.9575 - mae: 1.3248 - rmse: 1.8152 - val_loss: 4.8383 - val_mae: 1.4829 - val_rmse: 2.0396\n",
      "Epoch 71/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.9042 - mae: 1.3136 - rmse: 1.8016 - val_loss: 4.7908 - val_mae: 1.4762 - val_rmse: 2.0264\n",
      "Epoch 72/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.9263 - mae: 1.3247 - rmse: 1.8091 - val_loss: 4.8179 - val_mae: 1.4784 - val_rmse: 2.0357\n",
      "Epoch 73/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.8505 - mae: 1.3058 - rmse: 1.7896 - val_loss: 4.7424 - val_mae: 1.4690 - val_rmse: 2.0163\n",
      "Epoch 74/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.7705 - mae: 1.2958 - rmse: 1.7681 - val_loss: 4.8107 - val_mae: 1.4833 - val_rmse: 2.0420\n",
      "Epoch 75/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.7353 - mae: 1.2875 - rmse: 1.7614 - val_loss: 4.7266 - val_mae: 1.4691 - val_rmse: 2.0219\n",
      "Epoch 76/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.6791 - mae: 1.2792 - rmse: 1.7464 - val_loss: 4.6749 - val_mae: 1.4593 - val_rmse: 2.0109\n",
      "Epoch 77/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.6295 - mae: 1.2726 - rmse: 1.7341 - val_loss: 4.6446 - val_mae: 1.4551 - val_rmse: 2.0041\n",
      "Epoch 78/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.5938 - mae: 1.2660 - rmse: 1.7253 - val_loss: 4.6287 - val_mae: 1.4529 - val_rmse: 2.0031\n",
      "Epoch 79/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.5684 - mae: 1.2635 - rmse: 1.7192 - val_loss: 4.5061 - val_mae: 1.4335 - val_rmse: 1.9700\n",
      "Epoch 80/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.5080 - mae: 1.2499 - rmse: 1.7030 - val_loss: 4.4478 - val_mae: 1.4238 - val_rmse: 1.9562\n",
      "Epoch 81/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.4689 - mae: 1.2425 - rmse: 1.6925 - val_loss: 4.4878 - val_mae: 1.4353 - val_rmse: 1.9692\n",
      "Epoch 82/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.4442 - mae: 1.2395 - rmse: 1.6870 - val_loss: 4.5114 - val_mae: 1.4452 - val_rmse: 1.9802\n",
      "Epoch 83/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.4076 - mae: 1.2338 - rmse: 1.6782 - val_loss: 4.3831 - val_mae: 1.4194 - val_rmse: 1.9459\n",
      "Epoch 84/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.3973 - mae: 1.2310 - rmse: 1.6742 - val_loss: 4.2742 - val_mae: 1.4001 - val_rmse: 1.9138\n",
      "Epoch 85/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.3446 - mae: 1.2206 - rmse: 1.6604 - val_loss: 4.2718 - val_mae: 1.4054 - val_rmse: 1.9192\n",
      "Epoch 86/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.2652 - mae: 1.2112 - rmse: 1.6396 - val_loss: 4.2439 - val_mae: 1.4028 - val_rmse: 1.9153\n",
      "Epoch 87/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.2498 - mae: 1.2065 - rmse: 1.6358 - val_loss: 4.2111 - val_mae: 1.4000 - val_rmse: 1.9083\n",
      "Epoch 88/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.2228 - mae: 1.2021 - rmse: 1.6293 - val_loss: 4.1468 - val_mae: 1.3900 - val_rmse: 1.8908\n",
      "Epoch 89/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.1750 - mae: 1.1945 - rmse: 1.6168 - val_loss: 4.1324 - val_mae: 1.3856 - val_rmse: 1.8903\n",
      "Epoch 90/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.1348 - mae: 1.1890 - rmse: 1.6077 - val_loss: 4.0045 - val_mae: 1.3620 - val_rmse: 1.8518\n",
      "Epoch 91/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.1079 - mae: 1.1834 - rmse: 1.5990 - val_loss: 4.0497 - val_mae: 1.3767 - val_rmse: 1.8735\n",
      "Epoch 92/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.0718 - mae: 1.1743 - rmse: 1.5902 - val_loss: 4.0370 - val_mae: 1.3766 - val_rmse: 1.8708\n",
      "Epoch 93/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.0344 - mae: 1.1692 - rmse: 1.5804 - val_loss: 4.0161 - val_mae: 1.3770 - val_rmse: 1.8683\n",
      "Epoch 94/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.9842 - mae: 1.1612 - rmse: 1.5659 - val_loss: 3.9833 - val_mae: 1.3716 - val_rmse: 1.8619\n",
      "Epoch 95/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.9583 - mae: 1.1557 - rmse: 1.5611 - val_loss: 3.8612 - val_mae: 1.3472 - val_rmse: 1.8245\n",
      "Epoch 96/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.9542 - mae: 1.1606 - rmse: 1.5600 - val_loss: 3.8079 - val_mae: 1.3388 - val_rmse: 1.8081\n",
      "Epoch 97/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.9403 - mae: 1.1535 - rmse: 1.5544 - val_loss: 3.8284 - val_mae: 1.3433 - val_rmse: 1.8176\n",
      "Epoch 98/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.9065 - mae: 1.1492 - rmse: 1.5457 - val_loss: 3.8145 - val_mae: 1.3428 - val_rmse: 1.8181\n",
      "Epoch 99/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.8527 - mae: 1.1394 - rmse: 1.5299 - val_loss: 3.8052 - val_mae: 1.3423 - val_rmse: 1.8173\n",
      "Epoch 100/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.8129 - mae: 1.1292 - rmse: 1.5208 - val_loss: 3.7240 - val_mae: 1.3264 - val_rmse: 1.7917\n",
      "Epoch 101/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.7881 - mae: 1.1292 - rmse: 1.5140 - val_loss: 3.7289 - val_mae: 1.3298 - val_rmse: 1.7968\n",
      "Epoch 102/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.7558 - mae: 1.1225 - rmse: 1.5058 - val_loss: 3.6485 - val_mae: 1.3145 - val_rmse: 1.7749\n",
      "Epoch 103/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.7337 - mae: 1.1220 - rmse: 1.5021 - val_loss: 3.6653 - val_mae: 1.3222 - val_rmse: 1.7845\n",
      "Epoch 104/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.6853 - mae: 1.1124 - rmse: 1.4871 - val_loss: 3.6673 - val_mae: 1.3269 - val_rmse: 1.7892\n",
      "Epoch 105/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.6973 - mae: 1.1129 - rmse: 1.4932 - val_loss: 3.6137 - val_mae: 1.3164 - val_rmse: 1.7727\n",
      "Epoch 106/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.6667 - mae: 1.1070 - rmse: 1.4843 - val_loss: 3.7157 - val_mae: 1.3475 - val_rmse: 1.8093\n",
      "Epoch 107/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.6183 - mae: 1.0972 - rmse: 1.4704 - val_loss: 3.5498 - val_mae: 1.3042 - val_rmse: 1.7563\n",
      "Epoch 108/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.5949 - mae: 1.0922 - rmse: 1.4647 - val_loss: 3.5371 - val_mae: 1.3026 - val_rmse: 1.7547\n",
      "Epoch 109/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.5667 - mae: 1.0902 - rmse: 1.4574 - val_loss: 3.5696 - val_mae: 1.3160 - val_rmse: 1.7720\n",
      "Epoch 110/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.5801 - mae: 1.0901 - rmse: 1.4621 - val_loss: 3.4552 - val_mae: 1.2867 - val_rmse: 1.7309\n",
      "Epoch 111/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.5249 - mae: 1.0796 - rmse: 1.4439 - val_loss: 3.5078 - val_mae: 1.3044 - val_rmse: 1.7560\n",
      "Epoch 112/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.4955 - mae: 1.0756 - rmse: 1.4387 - val_loss: 3.4095 - val_mae: 1.2788 - val_rmse: 1.7251\n",
      "Epoch 113/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.4428 - mae: 1.0652 - rmse: 1.4231 - val_loss: 3.3859 - val_mae: 1.2774 - val_rmse: 1.7225\n",
      "Epoch 114/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.4399 - mae: 1.0667 - rmse: 1.4253 - val_loss: 3.3407 - val_mae: 1.2685 - val_rmse: 1.7083\n",
      "Epoch 115/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.4225 - mae: 1.0629 - rmse: 1.4191 - val_loss: 3.3450 - val_mae: 1.2718 - val_rmse: 1.7134\n",
      "Epoch 116/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.3989 - mae: 1.0601 - rmse: 1.4131 - val_loss: 3.3211 - val_mae: 1.2682 - val_rmse: 1.7092\n",
      "Epoch 117/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.3686 - mae: 1.0528 - rmse: 1.4048 - val_loss: 3.3108 - val_mae: 1.2699 - val_rmse: 1.7095\n",
      "Epoch 118/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.3566 - mae: 1.0502 - rmse: 1.4033 - val_loss: 3.2571 - val_mae: 1.2583 - val_rmse: 1.6928\n",
      "Epoch 119/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.3404 - mae: 1.0512 - rmse: 1.3982 - val_loss: 3.2574 - val_mae: 1.2600 - val_rmse: 1.6947\n",
      "Epoch 120/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.3097 - mae: 1.0417 - rmse: 1.3898 - val_loss: 3.2162 - val_mae: 1.2502 - val_rmse: 1.6850\n",
      "Epoch 121/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.2937 - mae: 1.0416 - rmse: 1.3863 - val_loss: 3.2201 - val_mae: 1.2536 - val_rmse: 1.6883\n",
      "Epoch 122/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.2704 - mae: 1.0368 - rmse: 1.3792 - val_loss: 3.1605 - val_mae: 1.2396 - val_rmse: 1.6682\n",
      "Epoch 123/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.2524 - mae: 1.0333 - rmse: 1.3736 - val_loss: 3.1779 - val_mae: 1.2491 - val_rmse: 1.6796\n",
      "Epoch 124/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.2146 - mae: 1.0236 - rmse: 1.3643 - val_loss: 3.1073 - val_mae: 1.2338 - val_rmse: 1.6587\n",
      "Epoch 125/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1963 - mae: 1.0216 - rmse: 1.3605 - val_loss: 3.1364 - val_mae: 1.2460 - val_rmse: 1.6746\n",
      "Epoch 126/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1687 - mae: 1.0172 - rmse: 1.3545 - val_loss: 3.0624 - val_mae: 1.2259 - val_rmse: 1.6488\n",
      "Epoch 127/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1627 - mae: 1.0173 - rmse: 1.3529 - val_loss: 3.0936 - val_mae: 1.2381 - val_rmse: 1.6651\n",
      "Epoch 128/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1456 - mae: 1.0138 - rmse: 1.3495 - val_loss: 3.0172 - val_mae: 1.2175 - val_rmse: 1.6385\n",
      "Epoch 129/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1184 - mae: 1.0096 - rmse: 1.3411 - val_loss: 3.0254 - val_mae: 1.2222 - val_rmse: 1.6464\n",
      "Epoch 130/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0990 - mae: 1.0045 - rmse: 1.3370 - val_loss: 2.9822 - val_mae: 1.2135 - val_rmse: 1.6344\n",
      "Epoch 131/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0875 - mae: 1.0037 - rmse: 1.3358 - val_loss: 2.9611 - val_mae: 1.2098 - val_rmse: 1.6303\n",
      "Epoch 132/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0653 - mae: 0.9999 - rmse: 1.3295 - val_loss: 2.9164 - val_mae: 1.2014 - val_rmse: 1.6163\n",
      "Epoch 133/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0560 - mae: 0.9989 - rmse: 1.3274 - val_loss: 2.9205 - val_mae: 1.2040 - val_rmse: 1.6221\n",
      "Epoch 134/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0373 - mae: 0.9935 - rmse: 1.3211 - val_loss: 2.8900 - val_mae: 1.1930 - val_rmse: 1.6069\n",
      "Epoch 135/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0281 - mae: 0.9928 - rmse: 1.3183 - val_loss: 2.8763 - val_mae: 1.1941 - val_rmse: 1.6081\n",
      "Epoch 136/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0021 - mae: 0.9882 - rmse: 1.3136 - val_loss: 2.8361 - val_mae: 1.1871 - val_rmse: 1.5982\n",
      "Epoch 137/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9755 - mae: 0.9859 - rmse: 1.3078 - val_loss: 2.8352 - val_mae: 1.1918 - val_rmse: 1.6049\n",
      "Epoch 138/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9600 - mae: 0.9799 - rmse: 1.3038 - val_loss: 2.8345 - val_mae: 1.1952 - val_rmse: 1.6094\n",
      "Epoch 139/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9427 - mae: 0.9805 - rmse: 1.3013 - val_loss: 2.7766 - val_mae: 1.1788 - val_rmse: 1.5880\n",
      "Epoch 140/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9165 - mae: 0.9735 - rmse: 1.2941 - val_loss: 2.7570 - val_mae: 1.1763 - val_rmse: 1.5847\n",
      "Epoch 141/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9066 - mae: 0.9736 - rmse: 1.2924 - val_loss: 2.7341 - val_mae: 1.1712 - val_rmse: 1.5790\n",
      "Epoch 142/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8883 - mae: 0.9708 - rmse: 1.2882 - val_loss: 2.7191 - val_mae: 1.1688 - val_rmse: 1.5765\n",
      "Epoch 143/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8720 - mae: 0.9673 - rmse: 1.2841 - val_loss: 2.7065 - val_mae: 1.1684 - val_rmse: 1.5754\n",
      "Epoch 144/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8621 - mae: 0.9666 - rmse: 1.2821 - val_loss: 2.7014 - val_mae: 1.1700 - val_rmse: 1.5780\n",
      "Epoch 145/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8445 - mae: 0.9635 - rmse: 1.2786 - val_loss: 2.6638 - val_mae: 1.1598 - val_rmse: 1.5660\n",
      "Epoch 146/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8281 - mae: 0.9605 - rmse: 1.2750 - val_loss: 2.6494 - val_mae: 1.1581 - val_rmse: 1.5644\n",
      "Epoch 147/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8166 - mae: 0.9581 - rmse: 1.2725 - val_loss: 2.6355 - val_mae: 1.1565 - val_rmse: 1.5607\n",
      "Epoch 148/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8024 - mae: 0.9553 - rmse: 1.2681 - val_loss: 2.6142 - val_mae: 1.1518 - val_rmse: 1.5544\n",
      "Epoch 149/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7932 - mae: 0.9545 - rmse: 1.2669 - val_loss: 2.6009 - val_mae: 1.1519 - val_rmse: 1.5548\n",
      "Epoch 150/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7788 - mae: 0.9520 - rmse: 1.2636 - val_loss: 2.6002 - val_mae: 1.1560 - val_rmse: 1.5595\n",
      "Epoch 151/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7586 - mae: 0.9504 - rmse: 1.2607 - val_loss: 2.5474 - val_mae: 1.1423 - val_rmse: 1.5421\n",
      "Epoch 152/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7401 - mae: 0.9465 - rmse: 1.2568 - val_loss: 2.5409 - val_mae: 1.1436 - val_rmse: 1.5444\n",
      "Epoch 153/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7282 - mae: 0.9459 - rmse: 1.2551 - val_loss: 2.5186 - val_mae: 1.1401 - val_rmse: 1.5387\n",
      "Epoch 154/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7111 - mae: 0.9428 - rmse: 1.2513 - val_loss: 2.5055 - val_mae: 1.1385 - val_rmse: 1.5363\n",
      "Epoch 155/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6959 - mae: 0.9414 - rmse: 1.2476 - val_loss: 2.4989 - val_mae: 1.1389 - val_rmse: 1.5377\n",
      "Epoch 156/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6816 - mae: 0.9379 - rmse: 1.2445 - val_loss: 2.4881 - val_mae: 1.1389 - val_rmse: 1.5383\n",
      "Epoch 157/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6693 - mae: 0.9374 - rmse: 1.2431 - val_loss: 2.4596 - val_mae: 1.1308 - val_rmse: 1.5280\n",
      "Epoch 158/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6582 - mae: 0.9355 - rmse: 1.2402 - val_loss: 2.4564 - val_mae: 1.1321 - val_rmse: 1.5293\n",
      "Epoch 159/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6478 - mae: 0.9341 - rmse: 1.2391 - val_loss: 2.4294 - val_mae: 1.1264 - val_rmse: 1.5218\n",
      "Epoch 160/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6333 - mae: 0.9333 - rmse: 1.2362 - val_loss: 2.4140 - val_mae: 1.1252 - val_rmse: 1.5204\n",
      "Epoch 161/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6158 - mae: 0.9294 - rmse: 1.2326 - val_loss: 2.3998 - val_mae: 1.1240 - val_rmse: 1.5187\n",
      "Epoch 162/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6045 - mae: 0.9289 - rmse: 1.2310 - val_loss: 2.3881 - val_mae: 1.1230 - val_rmse: 1.5174\n",
      "Epoch 163/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5945 - mae: 0.9272 - rmse: 1.2292 - val_loss: 2.3708 - val_mae: 1.1202 - val_rmse: 1.5134\n",
      "Epoch 164/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5850 - mae: 0.9261 - rmse: 1.2283 - val_loss: 2.3568 - val_mae: 1.1181 - val_rmse: 1.5102\n",
      "Epoch 165/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5712 - mae: 0.9243 - rmse: 1.2256 - val_loss: 2.3452 - val_mae: 1.1164 - val_rmse: 1.5086\n",
      "Epoch 166/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5596 - mae: 0.9227 - rmse: 1.2224 - val_loss: 2.3359 - val_mae: 1.1158 - val_rmse: 1.5078\n",
      "Epoch 167/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5462 - mae: 0.9214 - rmse: 1.2203 - val_loss: 2.3167 - val_mae: 1.1136 - val_rmse: 1.5050\n",
      "Epoch 168/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5347 - mae: 0.9204 - rmse: 1.2190 - val_loss: 2.3031 - val_mae: 1.1127 - val_rmse: 1.5039\n",
      "Epoch 169/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5219 - mae: 0.9190 - rmse: 1.2172 - val_loss: 2.2862 - val_mae: 1.1097 - val_rmse: 1.4993\n",
      "Epoch 170/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5132 - mae: 0.9184 - rmse: 1.2157 - val_loss: 2.2759 - val_mae: 1.1086 - val_rmse: 1.4978\n",
      "Epoch 171/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5025 - mae: 0.9167 - rmse: 1.2130 - val_loss: 2.2626 - val_mae: 1.1071 - val_rmse: 1.4952\n",
      "Epoch 172/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4901 - mae: 0.9155 - rmse: 1.2116 - val_loss: 2.2477 - val_mae: 1.1054 - val_rmse: 1.4925\n",
      "Epoch 173/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4800 - mae: 0.9143 - rmse: 1.2092 - val_loss: 2.2382 - val_mae: 1.1044 - val_rmse: 1.4911\n",
      "Epoch 174/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4709 - mae: 0.9132 - rmse: 1.2077 - val_loss: 2.2285 - val_mae: 1.1032 - val_rmse: 1.4895\n",
      "Epoch 175/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4626 - mae: 0.9121 - rmse: 1.2060 - val_loss: 2.2199 - val_mae: 1.1020 - val_rmse: 1.4876\n",
      "Epoch 176/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4562 - mae: 0.9110 - rmse: 1.2044 - val_loss: 2.2116 - val_mae: 1.1007 - val_rmse: 1.4854\n",
      "Epoch 177/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4502 - mae: 0.9105 - rmse: 1.2025 - val_loss: 2.2056 - val_mae: 1.0997 - val_rmse: 1.4838\n",
      "Epoch 178/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.4468 - mae: 0.9095 - rmse: 1.2010 - val_loss: 2.1996 - val_mae: 1.0987 - val_rmse: 1.4824\n",
      "Epoch 179/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 1.4408 - mae: 0.9084 - rmse: 1.1996 - val_loss: 2.1952 - val_mae: 1.0979 - val_rmse: 1.4810\n",
      "Epoch 180/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4394 - mae: 0.9078 - rmse: 1.1982 - val_loss: 2.1961 - val_mae: 1.0972 - val_rmse: 1.4798\n",
      "Epoch 181/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4357 - mae: 0.9071 - rmse: 1.1968 - val_loss: 2.1884 - val_mae: 1.0961 - val_rmse: 1.4782\n",
      "Epoch 182/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4318 - mae: 0.9062 - rmse: 1.1955 - val_loss: 2.1837 - val_mae: 1.0954 - val_rmse: 1.4772\n",
      "Epoch 183/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4295 - mae: 0.9055 - rmse: 1.1942 - val_loss: 2.1822 - val_mae: 1.0945 - val_rmse: 1.4759\n",
      "Epoch 184/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4263 - mae: 0.9048 - rmse: 1.1929 - val_loss: 2.1809 - val_mae: 1.0938 - val_rmse: 1.4749\n",
      "Epoch 185/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4249 - mae: 0.9040 - rmse: 1.1916 - val_loss: 2.1747 - val_mae: 1.0931 - val_rmse: 1.4736\n",
      "Epoch 186/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4202 - mae: 0.9034 - rmse: 1.1904 - val_loss: 2.1703 - val_mae: 1.0924 - val_rmse: 1.4727\n",
      "Epoch 187/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4159 - mae: 0.9025 - rmse: 1.1892 - val_loss: 2.1678 - val_mae: 1.0917 - val_rmse: 1.4716\n",
      "Epoch 188/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4123 - mae: 0.9016 - rmse: 1.1880 - val_loss: 2.1641 - val_mae: 1.0912 - val_rmse: 1.4709\n",
      "Epoch 189/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4092 - mae: 0.9011 - rmse: 1.1869 - val_loss: 2.1610 - val_mae: 1.0906 - val_rmse: 1.4699\n",
      "Epoch 190/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4064 - mae: 0.9004 - rmse: 1.1858 - val_loss: 2.1574 - val_mae: 1.0900 - val_rmse: 1.4687\n",
      "Epoch 191/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4037 - mae: 0.8997 - rmse: 1.1847 - val_loss: 2.1552 - val_mae: 1.0894 - val_rmse: 1.4679\n",
      "Epoch 192/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4014 - mae: 0.8990 - rmse: 1.1836 - val_loss: 2.1522 - val_mae: 1.0886 - val_rmse: 1.4669\n",
      "Epoch 193/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3996 - mae: 0.8983 - rmse: 1.1826 - val_loss: 2.1521 - val_mae: 1.0883 - val_rmse: 1.4662\n",
      "Epoch 194/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3997 - mae: 0.8977 - rmse: 1.1815 - val_loss: 2.1527 - val_mae: 1.0876 - val_rmse: 1.4651\n",
      "Epoch 195/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3970 - mae: 0.8969 - rmse: 1.1805 - val_loss: 2.1466 - val_mae: 1.0871 - val_rmse: 1.4642\n",
      "Epoch 196/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3941 - mae: 0.8965 - rmse: 1.1795 - val_loss: 2.1436 - val_mae: 1.0867 - val_rmse: 1.4634\n",
      "Epoch 197/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3914 - mae: 0.8961 - rmse: 1.1786 - val_loss: 2.1411 - val_mae: 1.0862 - val_rmse: 1.4626\n",
      "Epoch 198/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3889 - mae: 0.8954 - rmse: 1.1775 - val_loss: 2.1401 - val_mae: 1.0858 - val_rmse: 1.4619\n",
      "Epoch 199/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3864 - mae: 0.8947 - rmse: 1.1766 - val_loss: 2.1375 - val_mae: 1.0852 - val_rmse: 1.4609\n",
      "Epoch 200/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3845 - mae: 0.8943 - rmse: 1.1757 - val_loss: 2.1348 - val_mae: 1.0849 - val_rmse: 1.4604\n"
     ]
    }
   ],
   "source": [
    "model = LFactorNet(num_users=n_user, num_items=n_item, embedding_size=50)\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "    keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3), \n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping( \n",
    "    monitor='val_rmse', min_delta=0, patience=5, verbose=1, \n",
    "    mode='auto', baseline=None, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_pair,\n",
    "    y=train_rating,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    validation_split=.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.090626  5.8424387 4.087761  ... 3.3490307 6.1221333 3.8314614]\n",
      "rmse: LFactorNet: 1.400\n"
     ]
    }
   ],
   "source": [
    "## make prediction\n",
    "pred_rating = model.predict(test_pair).flatten()\n",
    "print(pred_rating)\n",
    "print('rmse: LFactorNet: %.3f' %np.sqrt(np.mean((pred_rating - test_rating)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6130528 4.703871  5.0403795 ... 3.0259562 8.935659  4.4722023]\n"
     ]
    }
   ],
   "source": [
    "pred_rating = model.predict(dtest_submit[[\"user_id\", \"item_id\"]]).flatten()\n",
    "print(pred_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({\"Id\": range(len(dtest_submit)), \"rating\": pred_rating})\n",
    "submit.to_csv(\"predict/submission1.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5129930097a52138fdc5ab816b09a2f27e944ad02f83c97d5e0e22f93f3b8c3c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
