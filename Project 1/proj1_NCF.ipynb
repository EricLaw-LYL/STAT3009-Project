{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT3009 Project 1\n",
    "- LAW Yiu Leung Eric 1155149315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the developed methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from developed_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the raw data\n",
    "- check the `user_id` and `item_id`: mapping `item_id` to a continuous sequence based on `sklean.preprocessing`\n",
    "- use `sklearn.model_selection.train_test_split` to generate `train` and `test` dataset\n",
    "- create `train_pair`,`train_rating`, `test_pair`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "dtest_submit = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "## mapping \n",
    "from sklearn import preprocessing\n",
    "le_user = preprocessing.LabelEncoder()\n",
    "le_user.fit(np.append(df['user_id'], dtest_submit[\"user_id\"]))\n",
    "df['user_id'] = le_user.transform(df[\"user_id\"])\n",
    "dtest_submit[\"user_id\"] = le_user.transform(dtest_submit[\"user_id\"])\n",
    "\n",
    "le_item = preprocessing.LabelEncoder()\n",
    "le_item.fit(np.append(df['item_id'], dtest_submit[\"item_id\"]))\n",
    "df[\"item_id\"] = le_item.transform(df[\"item_id\"])\n",
    "dtest_submit[\"item_id\"] = le_item.transform(dtest_submit[\"item_id\"])\n",
    "\n",
    "## generate train / test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtrain, dtest = train_test_split(df, test_size=0.33, random_state=42)\n",
    "\n",
    "## save real ratings for test set for evaluation.\n",
    "test_rating = np.array(dtest['rating'])\n",
    "\n",
    "## remove the ratings in the test set to simulate prediction\n",
    "dtest = dtest.drop(columns='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train_pair, train_rating and test_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pair, train_rating\n",
    "train_pair = dtrain[['user_id', 'item_id']].values\n",
    "train_rating = dtrain['rating'].values\n",
    "\n",
    "# test_pair\n",
    "test_pair = dtest[['user_id', 'item_id']].values\n",
    "n_user, n_item = max(train_pair[:,0].max(), test_pair[:,0].max())+1, max(train_pair[:,1].max(), test_pair[:,1].max())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impelement deep learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "We embed both users and movies in to 50-dimensional vectors.\n",
    "\n",
    "The model computes a match score between user and movie embeddings via a dot product, and adds a per-movie and per-user bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class LFactorNet(keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size, **kwargs):\n",
    "        super(LFactorNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "        self.glb_bias = tf.Variable(0., trainable=True) \n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_items,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.movie_bias = layers.Embedding(num_items, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
    "        # Add all the components (including bias)\n",
    "        x = dot_user_movie + user_bias + movie_bias + self.glb_bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "129/129 [==============================] - 2s 3ms/step - loss: 46.5726 - mae: 5.9808 - rmse: 6.7221 - val_loss: 43.9892 - val_mae: 5.7692 - val_rmse: 6.5327\n",
      "Epoch 2/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 34.2254 - mae: 4.8618 - rmse: 5.7271 - val_loss: 41.1163 - val_mae: 5.4971 - val_rmse: 6.2892\n",
      "Epoch 3/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 23.2300 - mae: 3.6397 - rmse: 4.6328 - val_loss: 36.5682 - val_mae: 5.0364 - val_rmse: 5.8803\n",
      "Epoch 4/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 14.9365 - mae: 2.5658 - rmse: 3.5700 - val_loss: 31.1090 - val_mae: 4.4544 - val_rmse: 5.3600\n",
      "Epoch 5/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 12.0649 - mae: 2.2642 - rmse: 3.1007 - val_loss: 28.3663 - val_mae: 4.1547 - val_rmse: 5.0867\n",
      "Epoch 6/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 11.5768 - mae: 2.2439 - rmse: 3.0178 - val_loss: 27.5207 - val_mae: 4.0769 - val_rmse: 5.0078\n",
      "Epoch 7/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 11.3855 - mae: 2.2304 - rmse: 2.9960 - val_loss: 26.8458 - val_mae: 4.0199 - val_rmse: 4.9474\n",
      "Epoch 8/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 11.1447 - mae: 2.2082 - rmse: 2.9681 - val_loss: 25.9613 - val_mae: 3.9367 - val_rmse: 4.8633\n",
      "Epoch 9/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.9683 - mae: 2.1893 - rmse: 2.9495 - val_loss: 25.1493 - val_mae: 3.8623 - val_rmse: 4.7862\n",
      "Epoch 10/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.7564 - mae: 2.1729 - rmse: 2.9254 - val_loss: 24.2897 - val_mae: 3.7807 - val_rmse: 4.7030\n",
      "Epoch 11/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.5006 - mae: 2.1507 - rmse: 2.8932 - val_loss: 23.4556 - val_mae: 3.7009 - val_rmse: 4.6213\n",
      "Epoch 12/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.2568 - mae: 2.1250 - rmse: 2.8623 - val_loss: 22.6511 - val_mae: 3.6235 - val_rmse: 4.5417\n",
      "Epoch 13/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 10.0819 - mae: 2.1044 - rmse: 2.8466 - val_loss: 21.3245 - val_mae: 3.4747 - val_rmse: 4.3982\n",
      "Epoch 14/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 9.8358 - mae: 2.0913 - rmse: 2.8124 - val_loss: 20.8755 - val_mae: 3.4419 - val_rmse: 4.3582\n",
      "Epoch 15/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 9.6728 - mae: 2.0666 - rmse: 2.7967 - val_loss: 19.8291 - val_mae: 3.3252 - val_rmse: 4.2438\n",
      "Epoch 16/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 9.4599 - mae: 2.0496 - rmse: 2.7703 - val_loss: 18.9242 - val_mae: 3.2266 - val_rmse: 4.1440\n",
      "Epoch 17/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 9.2722 - mae: 2.0290 - rmse: 2.7485 - val_loss: 17.4693 - val_mae: 3.0459 - val_rmse: 3.9689\n",
      "Epoch 18/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 9.0327 - mae: 2.0109 - rmse: 2.7158 - val_loss: 16.9872 - val_mae: 3.0006 - val_rmse: 3.9188\n",
      "Epoch 19/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.8742 - mae: 1.9998 - rmse: 2.6968 - val_loss: 16.4480 - val_mae: 2.9467 - val_rmse: 3.8602\n",
      "Epoch 20/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.7062 - mae: 1.9725 - rmse: 2.6784 - val_loss: 15.1271 - val_mae: 2.7747 - val_rmse: 3.6892\n",
      "Epoch 21/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.4944 - mae: 1.9518 - rmse: 2.6495 - val_loss: 14.3552 - val_mae: 2.6820 - val_rmse: 3.5913\n",
      "Epoch 22/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 8.3246 - mae: 1.9381 - rmse: 2.6276 - val_loss: 13.8000 - val_mae: 2.6186 - val_rmse: 3.5225\n",
      "Epoch 23/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 8.0847 - mae: 1.9092 - rmse: 2.5922 - val_loss: 13.1458 - val_mae: 2.5376 - val_rmse: 3.4367\n",
      "Epoch 24/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.9903 - mae: 1.8981 - rmse: 2.5841 - val_loss: 12.3894 - val_mae: 2.4400 - val_rmse: 3.3315\n",
      "Epoch 25/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.8421 - mae: 1.8856 - rmse: 2.5647 - val_loss: 11.6835 - val_mae: 2.3483 - val_rmse: 3.2309\n",
      "Epoch 26/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 7.6652 - mae: 1.8679 - rmse: 2.5388 - val_loss: 10.9579 - val_mae: 2.2504 - val_rmse: 3.1221\n",
      "Epoch 27/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.5112 - mae: 1.8508 - rmse: 2.5172 - val_loss: 10.5332 - val_mae: 2.2004 - val_rmse: 3.0614\n",
      "Epoch 28/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.3823 - mae: 1.8390 - rmse: 2.4981 - val_loss: 10.1435 - val_mae: 2.1520 - val_rmse: 3.0046\n",
      "Epoch 29/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 7.2303 - mae: 1.8121 - rmse: 2.4762 - val_loss: 9.7333 - val_mae: 2.1022 - val_rmse: 2.9420\n",
      "Epoch 30/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 7.0856 - mae: 1.7965 - rmse: 2.4547 - val_loss: 9.2740 - val_mae: 2.0445 - val_rmse: 2.8687\n",
      "Epoch 31/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.9752 - mae: 1.7855 - rmse: 2.4395 - val_loss: 9.0783 - val_mae: 2.0230 - val_rmse: 2.8419\n",
      "Epoch 32/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.8579 - mae: 1.7707 - rmse: 2.4204 - val_loss: 8.6300 - val_mae: 1.9674 - val_rmse: 2.7649\n",
      "Epoch 33/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.7031 - mae: 1.7492 - rmse: 2.3946 - val_loss: 8.3756 - val_mae: 1.9392 - val_rmse: 2.7242\n",
      "Epoch 34/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.6001 - mae: 1.7347 - rmse: 2.3783 - val_loss: 8.2095 - val_mae: 1.9222 - val_rmse: 2.6984\n",
      "Epoch 35/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.4996 - mae: 1.7266 - rmse: 2.3621 - val_loss: 7.8074 - val_mae: 1.8727 - val_rmse: 2.6231\n",
      "Epoch 36/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.3865 - mae: 1.7062 - rmse: 2.3415 - val_loss: 7.6485 - val_mae: 1.8560 - val_rmse: 2.5948\n",
      "Epoch 37/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.2908 - mae: 1.6941 - rmse: 2.3242 - val_loss: 7.4434 - val_mae: 1.8285 - val_rmse: 2.5582\n",
      "Epoch 38/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.1907 - mae: 1.6807 - rmse: 2.3069 - val_loss: 7.4118 - val_mae: 1.8241 - val_rmse: 2.5595\n",
      "Epoch 39/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 6.0973 - mae: 1.6768 - rmse: 2.2899 - val_loss: 7.2372 - val_mae: 1.8031 - val_rmse: 2.5267\n",
      "Epoch 40/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.9504 - mae: 1.6459 - rmse: 2.2602 - val_loss: 7.0207 - val_mae: 1.7779 - val_rmse: 2.4856\n",
      "Epoch 41/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.8706 - mae: 1.6310 - rmse: 2.2463 - val_loss: 6.8007 - val_mae: 1.7547 - val_rmse: 2.4404\n",
      "Epoch 42/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.8124 - mae: 1.6301 - rmse: 2.2360 - val_loss: 7.0436 - val_mae: 1.7822 - val_rmse: 2.5007\n",
      "Epoch 43/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 5.7207 - mae: 1.6112 - rmse: 2.2168 - val_loss: 6.7723 - val_mae: 1.7439 - val_rmse: 2.4443\n",
      "Epoch 44/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 5.6569 - mae: 1.5985 - rmse: 2.2041 - val_loss: 6.5228 - val_mae: 1.7144 - val_rmse: 2.3911\n",
      "Epoch 45/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 5.5682 - mae: 1.5895 - rmse: 2.1855 - val_loss: 6.4462 - val_mae: 1.7027 - val_rmse: 2.3766\n",
      "Epoch 46/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 5.4691 - mae: 1.5778 - rmse: 2.1649 - val_loss: 6.3761 - val_mae: 1.6925 - val_rmse: 2.3648\n",
      "Epoch 47/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.3825 - mae: 1.5615 - rmse: 2.1475 - val_loss: 6.2630 - val_mae: 1.6787 - val_rmse: 2.3428\n",
      "Epoch 48/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.3552 - mae: 1.5592 - rmse: 2.1412 - val_loss: 6.2593 - val_mae: 1.6806 - val_rmse: 2.3449\n",
      "Epoch 49/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.2711 - mae: 1.5425 - rmse: 2.1221 - val_loss: 6.1862 - val_mae: 1.6713 - val_rmse: 2.3299\n",
      "Epoch 50/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.1751 - mae: 1.5242 - rmse: 2.1026 - val_loss: 6.0019 - val_mae: 1.6510 - val_rmse: 2.2854\n",
      "Epoch 51/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.0893 - mae: 1.5144 - rmse: 2.0839 - val_loss: 5.9449 - val_mae: 1.6386 - val_rmse: 2.2756\n",
      "Epoch 52/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 5.0075 - mae: 1.5052 - rmse: 2.0665 - val_loss: 5.9398 - val_mae: 1.6368 - val_rmse: 2.2796\n",
      "Epoch 53/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.9715 - mae: 1.4954 - rmse: 2.0583 - val_loss: 5.8112 - val_mae: 1.6189 - val_rmse: 2.2478\n",
      "Epoch 54/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.9084 - mae: 1.4874 - rmse: 2.0434 - val_loss: 5.8833 - val_mae: 1.6326 - val_rmse: 2.2711\n",
      "Epoch 55/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 4.8546 - mae: 1.4728 - rmse: 2.0304 - val_loss: 5.7056 - val_mae: 1.6081 - val_rmse: 2.2284\n",
      "Epoch 56/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.7480 - mae: 1.4602 - rmse: 2.0050 - val_loss: 5.7455 - val_mae: 1.6102 - val_rmse: 2.2426\n",
      "Epoch 57/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.7041 - mae: 1.4491 - rmse: 1.9955 - val_loss: 5.5458 - val_mae: 1.5861 - val_rmse: 2.1943\n",
      "Epoch 58/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 4.6972 - mae: 1.4488 - rmse: 1.9946 - val_loss: 5.4627 - val_mae: 1.5724 - val_rmse: 2.1743\n",
      "Epoch 59/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 4.6047 - mae: 1.4361 - rmse: 1.9722 - val_loss: 5.4570 - val_mae: 1.5727 - val_rmse: 2.1759\n",
      "Epoch 60/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.5101 - mae: 1.4168 - rmse: 1.9502 - val_loss: 5.4226 - val_mae: 1.5680 - val_rmse: 2.1727\n",
      "Epoch 61/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 4.4438 - mae: 1.4102 - rmse: 1.9347 - val_loss: 5.3653 - val_mae: 1.5576 - val_rmse: 2.1578\n",
      "Epoch 62/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.3924 - mae: 1.3933 - rmse: 1.9237 - val_loss: 5.3781 - val_mae: 1.5549 - val_rmse: 2.1668\n",
      "Epoch 63/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 4.3372 - mae: 1.3913 - rmse: 1.9111 - val_loss: 5.3559 - val_mae: 1.5556 - val_rmse: 2.1618\n",
      "Epoch 64/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.2912 - mae: 1.3778 - rmse: 1.8989 - val_loss: 5.2753 - val_mae: 1.5444 - val_rmse: 2.1434\n",
      "Epoch 65/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.2395 - mae: 1.3712 - rmse: 1.8879 - val_loss: 5.2251 - val_mae: 1.5384 - val_rmse: 2.1333\n",
      "Epoch 66/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 4.1756 - mae: 1.3672 - rmse: 1.8713 - val_loss: 5.2340 - val_mae: 1.5397 - val_rmse: 2.1382\n",
      "Epoch 67/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.1308 - mae: 1.3527 - rmse: 1.8608 - val_loss: 5.1785 - val_mae: 1.5328 - val_rmse: 2.1249\n",
      "Epoch 68/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 4.0877 - mae: 1.3541 - rmse: 1.8492 - val_loss: 5.1964 - val_mae: 1.5396 - val_rmse: 2.1303\n",
      "Epoch 69/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 4.0453 - mae: 1.3400 - rmse: 1.8405 - val_loss: 4.9575 - val_mae: 1.5033 - val_rmse: 2.0681\n",
      "Epoch 70/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 3.9774 - mae: 1.3285 - rmse: 1.8224 - val_loss: 4.9441 - val_mae: 1.5011 - val_rmse: 2.0705\n",
      "Epoch 71/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 3.8948 - mae: 1.3181 - rmse: 1.8016 - val_loss: 4.9176 - val_mae: 1.4962 - val_rmse: 2.0667\n",
      "Epoch 72/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.8738 - mae: 1.3151 - rmse: 1.7966 - val_loss: 4.9439 - val_mae: 1.5025 - val_rmse: 2.0762\n",
      "Epoch 73/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.8177 - mae: 1.3034 - rmse: 1.7833 - val_loss: 4.7721 - val_mae: 1.4760 - val_rmse: 2.0284\n",
      "Epoch 74/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 3.8093 - mae: 1.3044 - rmse: 1.7826 - val_loss: 4.7537 - val_mae: 1.4716 - val_rmse: 2.0276\n",
      "Epoch 75/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.7660 - mae: 1.2972 - rmse: 1.7706 - val_loss: 4.7466 - val_mae: 1.4695 - val_rmse: 2.0302\n",
      "Epoch 76/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.7047 - mae: 1.2869 - rmse: 1.7551 - val_loss: 4.6521 - val_mae: 1.4552 - val_rmse: 2.0081\n",
      "Epoch 77/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.6497 - mae: 1.2705 - rmse: 1.7415 - val_loss: 4.5775 - val_mae: 1.4466 - val_rmse: 1.9885\n",
      "Epoch 78/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.6056 - mae: 1.2672 - rmse: 1.7298 - val_loss: 4.7017 - val_mae: 1.4685 - val_rmse: 2.0283\n",
      "Epoch 79/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.5782 - mae: 1.2635 - rmse: 1.7245 - val_loss: 4.4958 - val_mae: 1.4329 - val_rmse: 1.9712\n",
      "Epoch 80/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.5103 - mae: 1.2520 - rmse: 1.7054 - val_loss: 4.5660 - val_mae: 1.4485 - val_rmse: 1.9942\n",
      "Epoch 81/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.5099 - mae: 1.2579 - rmse: 1.7068 - val_loss: 4.4714 - val_mae: 1.4348 - val_rmse: 1.9686\n",
      "Epoch 82/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.4370 - mae: 1.2389 - rmse: 1.6860 - val_loss: 4.4658 - val_mae: 1.4357 - val_rmse: 1.9693\n",
      "Epoch 83/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 3.3811 - mae: 1.2309 - rmse: 1.6721 - val_loss: 4.4515 - val_mae: 1.4363 - val_rmse: 1.9699\n",
      "Epoch 84/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.3629 - mae: 1.2249 - rmse: 1.6688 - val_loss: 4.3972 - val_mae: 1.4279 - val_rmse: 1.9572\n",
      "Epoch 85/200\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 3.3190 - mae: 1.2189 - rmse: 1.6588 - val_loss: 4.2822 - val_mae: 1.4068 - val_rmse: 1.9225\n",
      "Epoch 86/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.2919 - mae: 1.2188 - rmse: 1.6508 - val_loss: 4.2409 - val_mae: 1.4015 - val_rmse: 1.9148\n",
      "Epoch 87/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.2648 - mae: 1.2121 - rmse: 1.6437 - val_loss: 4.2023 - val_mae: 1.3976 - val_rmse: 1.9060\n",
      "Epoch 88/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.2178 - mae: 1.2030 - rmse: 1.6306 - val_loss: 4.1820 - val_mae: 1.3991 - val_rmse: 1.9042\n",
      "Epoch 89/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.1896 - mae: 1.1975 - rmse: 1.6244 - val_loss: 4.1370 - val_mae: 1.3898 - val_rmse: 1.8931\n",
      "Epoch 90/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.1480 - mae: 1.1907 - rmse: 1.6129 - val_loss: 4.0591 - val_mae: 1.3774 - val_rmse: 1.8693\n",
      "Epoch 91/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.1201 - mae: 1.1870 - rmse: 1.6053 - val_loss: 4.0228 - val_mae: 1.3711 - val_rmse: 1.8602\n",
      "Epoch 92/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.0886 - mae: 1.1812 - rmse: 1.5961 - val_loss: 4.0111 - val_mae: 1.3718 - val_rmse: 1.8605\n",
      "Epoch 93/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.0735 - mae: 1.1804 - rmse: 1.5916 - val_loss: 3.9724 - val_mae: 1.3642 - val_rmse: 1.8503\n",
      "Epoch 94/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 3.0358 - mae: 1.1721 - rmse: 1.5822 - val_loss: 3.9099 - val_mae: 1.3558 - val_rmse: 1.8330\n",
      "Epoch 95/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.9984 - mae: 1.1677 - rmse: 1.5713 - val_loss: 3.9333 - val_mae: 1.3634 - val_rmse: 1.8455\n",
      "Epoch 96/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.9382 - mae: 1.1547 - rmse: 1.5559 - val_loss: 3.9073 - val_mae: 1.3595 - val_rmse: 1.8429\n",
      "Epoch 97/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.9071 - mae: 1.1488 - rmse: 1.5494 - val_loss: 3.8696 - val_mae: 1.3529 - val_rmse: 1.8331\n",
      "Epoch 98/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.8854 - mae: 1.1449 - rmse: 1.5444 - val_loss: 3.8808 - val_mae: 1.3580 - val_rmse: 1.8388\n",
      "Epoch 99/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.8585 - mae: 1.1429 - rmse: 1.5367 - val_loss: 3.8893 - val_mae: 1.3613 - val_rmse: 1.8459\n",
      "Epoch 100/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.8181 - mae: 1.1324 - rmse: 1.5259 - val_loss: 3.8092 - val_mae: 1.3468 - val_rmse: 1.8233\n",
      "Epoch 101/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.8017 - mae: 1.1315 - rmse: 1.5220 - val_loss: 3.7518 - val_mae: 1.3376 - val_rmse: 1.8055\n",
      "Epoch 102/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.7777 - mae: 1.1251 - rmse: 1.5152 - val_loss: 3.6981 - val_mae: 1.3269 - val_rmse: 1.7910\n",
      "Epoch 103/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.7418 - mae: 1.1197 - rmse: 1.5036 - val_loss: 3.7328 - val_mae: 1.3371 - val_rmse: 1.8062\n",
      "Epoch 104/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.7117 - mae: 1.1158 - rmse: 1.4971 - val_loss: 3.6336 - val_mae: 1.3145 - val_rmse: 1.7754\n",
      "Epoch 105/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.6867 - mae: 1.1115 - rmse: 1.4914 - val_loss: 3.6124 - val_mae: 1.3137 - val_rmse: 1.7734\n",
      "Epoch 106/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.6400 - mae: 1.1016 - rmse: 1.4776 - val_loss: 3.5783 - val_mae: 1.3083 - val_rmse: 1.7649\n",
      "Epoch 107/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.6231 - mae: 1.0993 - rmse: 1.4740 - val_loss: 3.5277 - val_mae: 1.2976 - val_rmse: 1.7515\n",
      "Epoch 108/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.6010 - mae: 1.0962 - rmse: 1.4684 - val_loss: 3.5032 - val_mae: 1.2940 - val_rmse: 1.7473\n",
      "Epoch 109/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.5744 - mae: 1.0945 - rmse: 1.4610 - val_loss: 3.4756 - val_mae: 1.2910 - val_rmse: 1.7423\n",
      "Epoch 110/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.5458 - mae: 1.0855 - rmse: 1.4531 - val_loss: 3.4498 - val_mae: 1.2890 - val_rmse: 1.7362\n",
      "Epoch 111/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.5159 - mae: 1.0812 - rmse: 1.4455 - val_loss: 3.4479 - val_mae: 1.2900 - val_rmse: 1.7388\n",
      "Epoch 112/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.4964 - mae: 1.0779 - rmse: 1.4411 - val_loss: 3.4308 - val_mae: 1.2896 - val_rmse: 1.7369\n",
      "Epoch 113/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.4647 - mae: 1.0701 - rmse: 1.4324 - val_loss: 3.3894 - val_mae: 1.2813 - val_rmse: 1.7258\n",
      "Epoch 114/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.4444 - mae: 1.0691 - rmse: 1.4271 - val_loss: 3.3668 - val_mae: 1.2780 - val_rmse: 1.7197\n",
      "Epoch 115/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.4377 - mae: 1.0671 - rmse: 1.4257 - val_loss: 3.3581 - val_mae: 1.2783 - val_rmse: 1.7197\n",
      "Epoch 116/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.4016 - mae: 1.0603 - rmse: 1.4143 - val_loss: 3.3467 - val_mae: 1.2787 - val_rmse: 1.7190\n",
      "Epoch 117/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.3658 - mae: 1.0518 - rmse: 1.4051 - val_loss: 3.2793 - val_mae: 1.2648 - val_rmse: 1.6992\n",
      "Epoch 118/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.3730 - mae: 1.0569 - rmse: 1.4096 - val_loss: 3.2662 - val_mae: 1.2591 - val_rmse: 1.6943\n",
      "Epoch 119/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.3352 - mae: 1.0482 - rmse: 1.3972 - val_loss: 3.2357 - val_mae: 1.2540 - val_rmse: 1.6874\n",
      "Epoch 120/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.3191 - mae: 1.0462 - rmse: 1.3947 - val_loss: 3.2089 - val_mae: 1.2517 - val_rmse: 1.6842\n",
      "Epoch 121/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.2785 - mae: 1.0359 - rmse: 1.3832 - val_loss: 3.1978 - val_mae: 1.2522 - val_rmse: 1.6859\n",
      "Epoch 122/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.2524 - mae: 1.0350 - rmse: 1.3780 - val_loss: 3.1569 - val_mae: 1.2444 - val_rmse: 1.6750\n",
      "Epoch 123/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.2352 - mae: 1.0298 - rmse: 1.3740 - val_loss: 3.1048 - val_mae: 1.2320 - val_rmse: 1.6585\n",
      "Epoch 124/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.2147 - mae: 1.0268 - rmse: 1.3693 - val_loss: 3.0862 - val_mae: 1.2302 - val_rmse: 1.6556\n",
      "Epoch 125/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1938 - mae: 1.0250 - rmse: 1.3634 - val_loss: 3.0772 - val_mae: 1.2284 - val_rmse: 1.6546\n",
      "Epoch 126/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1818 - mae: 1.0242 - rmse: 1.3604 - val_loss: 3.0602 - val_mae: 1.2261 - val_rmse: 1.6511\n",
      "Epoch 127/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1504 - mae: 1.0155 - rmse: 1.3518 - val_loss: 3.0384 - val_mae: 1.2220 - val_rmse: 1.6467\n",
      "Epoch 128/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1256 - mae: 1.0109 - rmse: 1.3457 - val_loss: 3.0203 - val_mae: 1.2195 - val_rmse: 1.6442\n",
      "Epoch 129/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.1158 - mae: 1.0103 - rmse: 1.3452 - val_loss: 3.0093 - val_mae: 1.2183 - val_rmse: 1.6434\n",
      "Epoch 130/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0979 - mae: 1.0075 - rmse: 1.3400 - val_loss: 2.9629 - val_mae: 1.2085 - val_rmse: 1.6299\n",
      "Epoch 131/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0783 - mae: 1.0054 - rmse: 1.3340 - val_loss: 2.9573 - val_mae: 1.2099 - val_rmse: 1.6305\n",
      "Epoch 132/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0733 - mae: 1.0046 - rmse: 1.3332 - val_loss: 2.9599 - val_mae: 1.2130 - val_rmse: 1.6334\n",
      "Epoch 133/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0462 - mae: 0.9963 - rmse: 1.3247 - val_loss: 2.8907 - val_mae: 1.1955 - val_rmse: 1.6104\n",
      "Epoch 134/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 2.0243 - mae: 0.9929 - rmse: 1.3199 - val_loss: 2.9046 - val_mae: 1.2046 - val_rmse: 1.6223\n",
      "Epoch 135/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9995 - mae: 0.9897 - rmse: 1.3153 - val_loss: 2.8841 - val_mae: 1.2027 - val_rmse: 1.6208\n",
      "Epoch 136/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9881 - mae: 0.9890 - rmse: 1.3142 - val_loss: 2.8281 - val_mae: 1.1864 - val_rmse: 1.6014\n",
      "Epoch 137/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9646 - mae: 0.9850 - rmse: 1.3084 - val_loss: 2.7977 - val_mae: 1.1803 - val_rmse: 1.5924\n",
      "Epoch 138/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9462 - mae: 0.9791 - rmse: 1.3035 - val_loss: 2.8012 - val_mae: 1.1849 - val_rmse: 1.5984\n",
      "Epoch 139/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9388 - mae: 0.9812 - rmse: 1.3030 - val_loss: 2.7733 - val_mae: 1.1784 - val_rmse: 1.5899\n",
      "Epoch 140/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.9213 - mae: 0.9762 - rmse: 1.2969 - val_loss: 2.7697 - val_mae: 1.1802 - val_rmse: 1.5925\n",
      "Epoch 141/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8955 - mae: 0.9711 - rmse: 1.2909 - val_loss: 2.7333 - val_mae: 1.1725 - val_rmse: 1.5813\n",
      "Epoch 142/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8852 - mae: 0.9725 - rmse: 1.2899 - val_loss: 2.7351 - val_mae: 1.1764 - val_rmse: 1.5856\n",
      "Epoch 143/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8739 - mae: 0.9676 - rmse: 1.2867 - val_loss: 2.6927 - val_mae: 1.1660 - val_rmse: 1.5700\n",
      "Epoch 144/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8554 - mae: 0.9633 - rmse: 1.2802 - val_loss: 2.6933 - val_mae: 1.1694 - val_rmse: 1.5759\n",
      "Epoch 145/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8409 - mae: 0.9626 - rmse: 1.2788 - val_loss: 2.6792 - val_mae: 1.1680 - val_rmse: 1.5756\n",
      "Epoch 146/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.8206 - mae: 0.9608 - rmse: 1.2750 - val_loss: 2.6318 - val_mae: 1.1564 - val_rmse: 1.5602\n",
      "Epoch 147/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.8117 - mae: 0.9588 - rmse: 1.2746 - val_loss: 2.6204 - val_mae: 1.1539 - val_rmse: 1.5587\n",
      "Epoch 148/200\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 1.7995 - mae: 0.9569 - rmse: 1.2708 - val_loss: 2.6079 - val_mae: 1.1527 - val_rmse: 1.5567\n",
      "Epoch 149/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7751 - mae: 0.9532 - rmse: 1.2650 - val_loss: 2.5901 - val_mae: 1.1507 - val_rmse: 1.5551\n",
      "Epoch 150/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7559 - mae: 0.9501 - rmse: 1.2615 - val_loss: 2.5662 - val_mae: 1.1470 - val_rmse: 1.5496\n",
      "Epoch 151/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7467 - mae: 0.9497 - rmse: 1.2608 - val_loss: 2.5459 - val_mae: 1.1426 - val_rmse: 1.5442\n",
      "Epoch 152/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7315 - mae: 0.9474 - rmse: 1.2575 - val_loss: 2.5379 - val_mae: 1.1435 - val_rmse: 1.5457\n",
      "Epoch 153/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7155 - mae: 0.9454 - rmse: 1.2540 - val_loss: 2.5138 - val_mae: 1.1382 - val_rmse: 1.5381\n",
      "Epoch 154/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.7074 - mae: 0.9435 - rmse: 1.2526 - val_loss: 2.5079 - val_mae: 1.1401 - val_rmse: 1.5405\n",
      "Epoch 155/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6910 - mae: 0.9415 - rmse: 1.2494 - val_loss: 2.4909 - val_mae: 1.1366 - val_rmse: 1.5365\n",
      "Epoch 156/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6751 - mae: 0.9394 - rmse: 1.2464 - val_loss: 2.4685 - val_mae: 1.1328 - val_rmse: 1.5308\n",
      "Epoch 157/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6615 - mae: 0.9375 - rmse: 1.2426 - val_loss: 2.4586 - val_mae: 1.1333 - val_rmse: 1.5314\n",
      "Epoch 158/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6510 - mae: 0.9359 - rmse: 1.2412 - val_loss: 2.4432 - val_mae: 1.1306 - val_rmse: 1.5283\n",
      "Epoch 159/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6450 - mae: 0.9339 - rmse: 1.2390 - val_loss: 2.4387 - val_mae: 1.1296 - val_rmse: 1.5280\n",
      "Epoch 160/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6256 - mae: 0.9317 - rmse: 1.2356 - val_loss: 2.4123 - val_mae: 1.1265 - val_rmse: 1.5240\n",
      "Epoch 161/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.6068 - mae: 0.9298 - rmse: 1.2335 - val_loss: 2.3952 - val_mae: 1.1233 - val_rmse: 1.5197\n",
      "Epoch 162/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5963 - mae: 0.9290 - rmse: 1.2317 - val_loss: 2.3788 - val_mae: 1.1219 - val_rmse: 1.5171\n",
      "Epoch 163/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5817 - mae: 0.9267 - rmse: 1.2282 - val_loss: 2.3698 - val_mae: 1.1224 - val_rmse: 1.5177\n",
      "Epoch 164/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5707 - mae: 0.9260 - rmse: 1.2273 - val_loss: 2.3487 - val_mae: 1.1173 - val_rmse: 1.5106\n",
      "Epoch 165/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5591 - mae: 0.9242 - rmse: 1.2245 - val_loss: 2.3363 - val_mae: 1.1160 - val_rmse: 1.5094\n",
      "Epoch 166/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5461 - mae: 0.9222 - rmse: 1.2218 - val_loss: 2.3243 - val_mae: 1.1148 - val_rmse: 1.5081\n",
      "Epoch 167/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5382 - mae: 0.9216 - rmse: 1.2208 - val_loss: 2.3100 - val_mae: 1.1128 - val_rmse: 1.5052\n",
      "Epoch 168/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5265 - mae: 0.9197 - rmse: 1.2186 - val_loss: 2.2971 - val_mae: 1.1112 - val_rmse: 1.5028\n",
      "Epoch 169/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5155 - mae: 0.9189 - rmse: 1.2169 - val_loss: 2.2832 - val_mae: 1.1089 - val_rmse: 1.4994\n",
      "Epoch 170/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.5055 - mae: 0.9175 - rmse: 1.2146 - val_loss: 2.2696 - val_mae: 1.1074 - val_rmse: 1.4972\n",
      "Epoch 171/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4946 - mae: 0.9163 - rmse: 1.2126 - val_loss: 2.2587 - val_mae: 1.1061 - val_rmse: 1.4954\n",
      "Epoch 172/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4891 - mae: 0.9151 - rmse: 1.2110 - val_loss: 2.2571 - val_mae: 1.1051 - val_rmse: 1.4943\n",
      "Epoch 173/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4795 - mae: 0.9144 - rmse: 1.2092 - val_loss: 2.2379 - val_mae: 1.1036 - val_rmse: 1.4918\n",
      "Epoch 174/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4699 - mae: 0.9130 - rmse: 1.2073 - val_loss: 2.2307 - val_mae: 1.1027 - val_rmse: 1.4902\n",
      "Epoch 175/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4610 - mae: 0.9118 - rmse: 1.2058 - val_loss: 2.2203 - val_mae: 1.1015 - val_rmse: 1.4881\n",
      "Epoch 176/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4542 - mae: 0.9114 - rmse: 1.2040 - val_loss: 2.2122 - val_mae: 1.1005 - val_rmse: 1.4864\n",
      "Epoch 177/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4480 - mae: 0.9103 - rmse: 1.2025 - val_loss: 2.2057 - val_mae: 1.0995 - val_rmse: 1.4846\n",
      "Epoch 178/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4434 - mae: 0.9097 - rmse: 1.2009 - val_loss: 2.2011 - val_mae: 1.0988 - val_rmse: 1.4832\n",
      "Epoch 179/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4400 - mae: 0.9088 - rmse: 1.1995 - val_loss: 2.1969 - val_mae: 1.0980 - val_rmse: 1.4819\n",
      "Epoch 180/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4366 - mae: 0.9080 - rmse: 1.1981 - val_loss: 2.1941 - val_mae: 1.0971 - val_rmse: 1.4806\n",
      "Epoch 181/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4375 - mae: 0.9070 - rmse: 1.1966 - val_loss: 2.1968 - val_mae: 1.0966 - val_rmse: 1.4796\n",
      "Epoch 182/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4342 - mae: 0.9065 - rmse: 1.1954 - val_loss: 2.1890 - val_mae: 1.0958 - val_rmse: 1.4784\n",
      "Epoch 183/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4273 - mae: 0.9055 - rmse: 1.1940 - val_loss: 2.1825 - val_mae: 1.0950 - val_rmse: 1.4770\n",
      "Epoch 184/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4233 - mae: 0.9047 - rmse: 1.1928 - val_loss: 2.1802 - val_mae: 1.0943 - val_rmse: 1.4761\n",
      "Epoch 185/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4212 - mae: 0.9040 - rmse: 1.1915 - val_loss: 2.1776 - val_mae: 1.0937 - val_rmse: 1.4750\n",
      "Epoch 186/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4185 - mae: 0.9032 - rmse: 1.1904 - val_loss: 2.1738 - val_mae: 1.0931 - val_rmse: 1.4741\n",
      "Epoch 187/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4149 - mae: 0.9025 - rmse: 1.1892 - val_loss: 2.1711 - val_mae: 1.0924 - val_rmse: 1.4731\n",
      "Epoch 188/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4136 - mae: 0.9019 - rmse: 1.1880 - val_loss: 2.1698 - val_mae: 1.0918 - val_rmse: 1.4721\n",
      "Epoch 189/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4114 - mae: 0.9011 - rmse: 1.1868 - val_loss: 2.1662 - val_mae: 1.0912 - val_rmse: 1.4711\n",
      "Epoch 190/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4087 - mae: 0.9006 - rmse: 1.1858 - val_loss: 2.1637 - val_mae: 1.0905 - val_rmse: 1.4701\n",
      "Epoch 191/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4051 - mae: 0.8997 - rmse: 1.1846 - val_loss: 2.1611 - val_mae: 1.0900 - val_rmse: 1.4693\n",
      "Epoch 192/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4033 - mae: 0.8992 - rmse: 1.1836 - val_loss: 2.1576 - val_mae: 1.0893 - val_rmse: 1.4683\n",
      "Epoch 193/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4014 - mae: 0.8985 - rmse: 1.1825 - val_loss: 2.1587 - val_mae: 1.0889 - val_rmse: 1.4676\n",
      "Epoch 194/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.4006 - mae: 0.8978 - rmse: 1.1815 - val_loss: 2.1550 - val_mae: 1.0883 - val_rmse: 1.4664\n",
      "Epoch 195/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3994 - mae: 0.8971 - rmse: 1.1804 - val_loss: 2.1534 - val_mae: 1.0877 - val_rmse: 1.4656\n",
      "Epoch 196/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3953 - mae: 0.8966 - rmse: 1.1795 - val_loss: 2.1483 - val_mae: 1.0872 - val_rmse: 1.4647\n",
      "Epoch 197/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3915 - mae: 0.8960 - rmse: 1.1785 - val_loss: 2.1450 - val_mae: 1.0868 - val_rmse: 1.4641\n",
      "Epoch 198/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3877 - mae: 0.8952 - rmse: 1.1776 - val_loss: 2.1416 - val_mae: 1.0864 - val_rmse: 1.4632\n",
      "Epoch 199/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3847 - mae: 0.8949 - rmse: 1.1766 - val_loss: 2.1393 - val_mae: 1.0859 - val_rmse: 1.4625\n",
      "Epoch 200/200\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 1.3828 - mae: 0.8943 - rmse: 1.1757 - val_loss: 2.1368 - val_mae: 1.0854 - val_rmse: 1.4616\n"
     ]
    }
   ],
   "source": [
    "model = LFactorNet(num_users=n_user, num_items=n_item, embedding_size=50)\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "    keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3), \n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping( \n",
    "    monitor='val_rmse', min_delta=0, patience=5, verbose=1, \n",
    "    mode='auto', baseline=None, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_pair,\n",
    "    y=train_rating,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    validation_split=.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0982437 5.8481407 4.09268   ... 3.3440409 6.140058  3.8438263]\n",
      "rmse: LFactorNet: 1.401\n"
     ]
    }
   ],
   "source": [
    "## make prediction\n",
    "pred_rating = model.predict(test_pair).flatten()\n",
    "print(pred_rating)\n",
    "print('rmse: LFactorNet: %.3f' %np.sqrt(np.mean((pred_rating - test_rating)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6130528 4.703871  5.0403795 ... 3.0259562 8.935659  4.4722023]\n"
     ]
    }
   ],
   "source": [
    "pred_rating = model.predict(dtest_submit[[\"user_id\", \"item_id\"]]).flatten()\n",
    "print(pred_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({\"Id\": range(len(dtest_submit)), \"rating\": pred_rating})\n",
    "submit.to_csv(\"predict/submission1.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5129930097a52138fdc5ab816b09a2f27e944ad02f83c97d5e0e22f93f3b8c3c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
